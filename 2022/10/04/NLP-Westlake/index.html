<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"moriarty0923.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="NLP —— Westlake 西湖大学张岳NLP教程">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP ---- Westlake">
<meta property="og:url" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/index.html">
<meta property="og:site_name" content="Moriarty&#39; blog">
<meta property="og:description" content="NLP —— Westlake 西湖大学张岳NLP教程">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20220924152854633.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20220926225354087.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20220926225757505.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20220927230300022.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20220927233250125.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20220927233747510.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20220927234053599.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20220929212906928.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20220929214219674.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20220929214618814.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20220929214904593.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20220929233540029.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20220930234129959.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221001235301852.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221001235725478.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221002004047241.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221002124356663.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221002124553512.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221002144729586.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221002145405032.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221002145603687.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221002180355379.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221002181634799.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221002193639504.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221002193851332.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221002194255807.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221002194828864.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221002195201349.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221002195444476.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221003123042744.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221003132424988.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221003132808497.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221003132833897.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221003135909337.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221003140320704.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221003140555278.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221003140745816.png">
<meta property="article:published_time" content="2022-10-03T16:23:08.156Z">
<meta property="article:modified_time" content="2022-10-05T17:41:40.039Z">
<meta property="article:author" content="Moriarty">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20220924152854633.png">

<link rel="canonical" href="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>NLP ---- Westlake | Moriarty' blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Moriarty' blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Moriarty">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Moriarty' blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          NLP ---- Westlake
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-10-04 00:23:08" itemprop="dateCreated datePublished" datetime="2022-10-04T00:23:08+08:00">2022-10-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-10-06 01:41:40" itemprop="dateModified" datetime="2022-10-06T01:41:40+08:00">2022-10-06</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><strong>NLP —— Westlake</strong></p>
<p><strong>西湖大学张岳NLP教程</strong></p>
<span id="more"></span>

<h1 id="chapter-1"><a href="#chapter-1" class="headerlink" title="chapter 1"></a><strong>chapter 1</strong></h1><h2 id="1-1-what-is-nlp"><a href="#1-1-what-is-nlp" class="headerlink" title="1.1 what is  nlp"></a>1.1 what is  nlp</h2><ul>
<li><h6 id="自动的理解和生成人类语言"><a href="#自动的理解和生成人类语言" class="headerlink" title="自动的理解和生成人类语言"></a>自动的理解和生成人类语言</h6></li>
</ul>
<h3 id="Main-approaches"><a href="#Main-approaches" class="headerlink" title="Main approaches"></a>Main approaches</h3><ul>
<li><h6 id="Rule-based-symbolic-approach-基于规则的方法"><a href="#Rule-based-symbolic-approach-基于规则的方法" class="headerlink" title="Rule-based(symbolic) approach  基于规则的方法"></a>Rule-based(symbolic) approach  基于规则的方法</h6></li>
<li><p><strong>Statistical approach(traditional machine learning) 基于统计的方法</strong></p>
</li>
<li><p><strong>Connectionist approach(Neural networks) 神经网络</strong></p>
</li>
</ul>
<h2 id="1-2-NLP-tasks"><a href="#1-2-NLP-tasks" class="headerlink" title="1.2 NLP  tasks"></a>1.2 NLP  tasks</h2><h3 id="1-2-1-Fundamental-Tasks"><a href="#1-2-1-Fundamental-Tasks" class="headerlink" title="1.2.1 Fundamental Tasks"></a>1.2.1 Fundamental Tasks</h3><h4 id="Syntactic-tasks-句法分析任务"><a href="#Syntactic-tasks-句法分析任务" class="headerlink" title="Syntactic tasks 句法分析任务"></a>Syntactic tasks 句法分析任务</h4><ul>
<li><strong>word level</strong>  <ul>
<li>Morphological analysis 形态化分析 <ul>
<li>词根和词缀提取分析 walking  –&gt; walk + ing</li>
</ul>
</li>
<li>word segmentation 分词（中文）<ul>
<li>字的序列分割成词的序列</li>
</ul>
</li>
<li>tokenization 分词 （英文）</li>
<li>POS  tagging 词性标注</li>
</ul>
</li>
<li><strong>sentence level 句子结构</strong><ul>
<li>Constituent parsing 成分句法 (层次短语句法)<ul>
<li>通过层次化的短语结构来表达一句话</li>
</ul>
</li>
<li>Dependency parsing 依存句法<ul>
<li>通过两个词的关系来组成结构</li>
<li>每个词修饰句中唯一一个词</li>
<li>有一次词不修饰任何词，root节点</li>
<li>树结构</li>
</ul>
</li>
<li>CCG parsing 组合范畴句法<ul>
<li>高度词汇化的句法</li>
<li>每一个词都有一个复杂的词汇化标签</li>
<li>CCG supertagging</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Semantic-task-语义分析任务"><a href="#Semantic-task-语义分析任务" class="headerlink" title="Semantic task 语义分析任务"></a>Semantic task 语义分析任务</h4><ul>
<li><strong>word level</strong>  <ul>
<li>Word sense disambiguation (WSD) 语义消歧</li>
<li>Metaphor 隐喻检测</li>
<li>Sense relations between words 词之间的关系</li>
</ul>
</li>
<li><strong>sentence level</strong><ul>
<li>Predicate-argument relations 谓词论元结构</li>
</ul>
</li>
</ul>
<h4 id="Discourse-tasks-篇章分析"><a href="#Discourse-tasks-篇章分析" class="headerlink" title="Discourse tasks 篇章分析"></a>Discourse tasks <strong>篇章分析</strong></h4><ul>
<li>discourse segmentation 篇章切分</li>
</ul>
<h3 id="1-2-2-Information-extraction-信息抽取"><a href="#1-2-2-Information-extraction-信息抽取" class="headerlink" title="1.2.2 Information extraction 信息抽取"></a>1.2.2 Information extraction 信息抽取</h3><ul>
<li><h6 id="Entities"><a href="#Entities" class="headerlink" title="Entities"></a><strong>Entities</strong></h6><ul>
<li>Named entity recognition 命名实体识别</li>
<li>Anaphora 指代消解</li>
<li>co-references 共指消解</li>
</ul>
</li>
<li><p><strong>Relations</strong></p>
<ul>
<li>relation extraction</li>
</ul>
</li>
<li><p><strong>Knowlege</strong> <strong>graph</strong> <strong>知识图谱</strong></p>
<p>a type of db , entities form nodes and relations form edges</p>
<ul>
<li>entity linking 实体链接</li>
<li>named entity normalization 命名实体规范化</li>
<li>link prediction 链接预测</li>
</ul>
</li>
<li><p><strong>Events</strong></p>
<ul>
<li>News event detection</li>
<li>Event factuality prediction 事件真实性检测</li>
<li>Event time extration 时间线检测</li>
<li>causality detection 事件因果关系</li>
<li>scipt learning 脚本学习</li>
</ul>
</li>
<li><p><strong>Sentiment</strong> <strong>analysis</strong></p>
<ul>
<li>Sentiment classification 情感分类</li>
<li>Targeted sentiment 基于对象的情感</li>
<li>Aspect-oriented sentiment 基于方面的情感</li>
<li>More fine-grained sentiment classification 更复杂的序列化情感</li>
<li>Sentiment detection</li>
<li>sentiment lexicon acquisition</li>
<li>emotion detection</li>
<li>stance detection and argumentation mining 立场检测</li>
</ul>
</li>
</ul>
<h3 id="1-2-3-Text-Generation-Tasks"><a href="#1-2-3-Text-Generation-Tasks" class="headerlink" title="1.2.3 Text Generation Tasks"></a>1.2.3 Text Generation Tasks</h3><ul>
<li><strong>Realization 实现</strong>   <ul>
<li>语义到文字的生成</li>
</ul>
</li>
<li><strong>Data-to-text Generation</strong></li>
<li><strong>Summarization</strong></li>
<li><strong>Machine</strong> <strong>translation</strong></li>
<li><strong>Grammar error correction</strong></li>
<li><strong>Question answering</strong></li>
<li><strong>Dialogue</strong> <strong>systems</strong></li>
</ul>
<h3 id="1-2-4-other-applications"><a href="#1-2-4-other-applications" class="headerlink" title="1.2.4 other applications"></a>1.2.4 other applications</h3><ul>
<li><strong>Information retrieval</strong> 信息检索</li>
<li><strong>Recommendation system</strong></li>
<li><strong>Text mining and text analytics</strong></li>
</ul>
<h2 id="1-3-NLP-from-a-Machine-Learning-Perspective"><a href="#1-3-NLP-from-a-Machine-Learning-Perspective" class="headerlink" title="1.3 NLP from a Machine Learning Perspective"></a>1.3 NLP from a Machine Learning Perspective</h2><p><img src="/2022/10/04/NLP-Westlake/image-20220924152854633.png" alt="image-20220924152854633"></p>
<ul>
<li>语言学特点</li>
<li>机器学习特征</li>
<li>数据特征</li>
</ul>
<p><strong>Machine</strong> <strong>learning perspective</strong></p>
<p>从机器学习的角度对NLP任务进行分类</p>
<ul>
<li><strong>Classification</strong> </li>
<li><strong>Structure</strong> <strong>prediction</strong> <strong>结构预测</strong></li>
<li><strong>Regression</strong></li>
</ul>
<p>从训练数据角度</p>
<ul>
<li>无监督学习</li>
<li>有监督学习</li>
<li>半监督学习</li>
</ul>
<h1 id="Chapter-2"><a href="#Chapter-2" class="headerlink" title="Chapter 2"></a><strong>Chapter</strong> <strong>2</strong></h1><h2 id="2-1-what-is-model"><a href="#2-1-what-is-model" class="headerlink" title="2.1 what is model"></a>2.1 what is model</h2><p><strong>Models</strong></p>
<p>​	是对一个特定任务或事件的抽象，使数学计算变得可行。</p>
<p><strong>Probabilistic</strong> <strong>model</strong>  概率模型</p>
<p>​	随机事件出现的可能性</p>
<h3 id="2-1-1-MLE"><a href="#2-1-1-MLE" class="headerlink" title="2.1.1 MLE"></a>2.1.1 MLE</h3><p>​	Maximum Likelihood  Estimation</p>
<p>​	最大似然估算</p>
<p> <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/148968222">https://zhuanlan.zhihu.com/p/148968222</a></p>
<h2 id="2-2-N-gram-Language-Models"><a href="#2-2-N-gram-Language-Models" class="headerlink" title="2.2 N-gram Language Models"></a>2.2 N-gram Language Models</h2><p><strong>n元语言模型</strong></p>
<p>语言模型 : 衡量一句自然语言的概率的模型</p>
<p>eg : P (“eat pizza” ) &gt; P ( “drink pizza” )</p>
<p><strong>N-gram</strong> </p>
<p>​	通过n元组来进行句子概率的计算</p>
<p>​	<strong>假定一个词出现的概率只与它前面固定数目的词相关</strong></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/114538417">https://zhuanlan.zhihu.com/p/114538417</a></p>
<p><strong>Unigram Language Models</strong>  </p>
<p>​	通过单个单词的概率计算句子的概率</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20220926225354087.png" alt="image-20220926225354087"></p>
<p><strong>OOV</strong> : out - of - vocabulary </p>
<ul>
<li>not seen in the training data</li>
<li>p(oov) &#x3D; 0</li>
<li>p(s) &#x3D;0  if oov in s</li>
</ul>
<p><strong>add-one smoothing</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20220926225757505.png" alt="image-20220926225757505"></p>
<p><strong>Hyper - parameter</strong> 超参</p>
<ul>
<li>fixed in advance and not trained during train</li>
<li>can be tuned , selected empirically to improve performance</li>
</ul>
<h3 id="2-2-2-Bigram-Language-Models"><a href="#2-2-2-Bigram-Language-Models" class="headerlink" title="2.2.2 Bigram Language Models"></a>2.2.2 Bigram Language Models</h3><p>二元语言模型</p>
<p> 一元语言模型无法解决：</p>
<p>“he ate pizza”   or   “he drank pizza”</p>
<p>二元语言模型计算条件概率 <strong>conditional probailities</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20220927230300022.png" alt="image-20220927230300022"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20220927233250125.png" alt="image-20220927233250125"></p>
<p><strong>sparsity</strong> : 稀疏性</p>
<p>测试集中的二元组没有在词表中出现过</p>
<p><strong>解决方法 ： Back-off</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20220927233747510.png" alt="image-20220927233747510"></p>
<p><strong>句子概率的计算</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20220927234053599.png" alt="image-20220927234053599"></p>
<h3 id="2-2-3-Trigram-Language-Models-and-Beyond"><a href="#2-2-3-Trigram-Language-Models-and-Beyond" class="headerlink" title="2.2.3 Trigram Language Models and Beyond"></a>2.2.3 Trigram Language Models and Beyond</h3><p><strong>假设概率只与前两个词有关</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20220929212906928.png" alt="image-20220929212906928"></p>
<h3 id="Methods-to-address-sparsity"><a href="#Methods-to-address-sparsity" class="headerlink" title="Methods to address sparsity"></a>Methods to address sparsity</h3><ul>
<li><p>add-one smoothing : add one to the count of all words </p>
</li>
<li><p>add-α smoothing : add α to the count of all words </p>
</li>
<li><p>back-off : use lower order n-gram probabilities to approximate high order n-gram probabilities</p>
</li>
<li><p>Good-Turing smoothing : make a rational guess of the count of OOV words</p>
</li>
<li><p>Knesser-Ney smoothing : work with back-off,consider the history context of lower order n-gram</p>
</li>
</ul>
<h3 id="Log-probability-models"><a href="#Log-probability-models" class="headerlink" title="Log-probability models"></a>Log-probability models</h3><p><img src="/2022/10/04/NLP-Westlake/image-20220929214219674.png" alt="image-20220929214219674"></p>
<p><strong>解决浮点数精度问题</strong></p>
<h3 id="2-2-4-Generative-Models"><a href="#2-2-4-Generative-Models" class="headerlink" title="2.2.4 Generative Models"></a>2.2.4 Generative Models</h3><p>生成模型 ： 把一句话里的每个词逐一的生成出来</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20220929214618814.png" alt="image-20220929214618814"></p>
<p>马尔可夫模型</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20220929214904593.png" alt="image-20220929214904593"></p>
<h2 id="2-3-Naive-Bayes-rule"><a href="#2-3-Naive-Bayes-rule" class="headerlink" title="2.3 Naive Bayes rule"></a>2.3 Naive Bayes rule</h2><p>We have </p>
<p>​	<strong>P(AB) &#x3D; P(A|B) P(B) &#x3D; P(B|A) P(A)</strong></p>
<p>Therefore</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20220929233540029.png" alt="image-20220929233540029"></p>
<p>–Bayes rule</p>
<h3 id="2-3-2-Evaluating-a-Text-Classifier"><a href="#2-3-2-Evaluating-a-Text-Classifier" class="headerlink" title="2.3.2 Evaluating a Text Classifier"></a>2.3.2 Evaluating a Text Classifier</h3><p><strong>Data:</strong></p>
<ul>
<li>Traing set </li>
<li>Test set</li>
<li>Development set  调整超参</li>
</ul>
<p><img src="/2022/10/04/NLP-Westlake/image-20220930234129959.png" alt="image-20220930234129959"></p>
<p>Evaluation  metric</p>
<ul>
<li>Accuracy</li>
</ul>
<p>$$<br>Acc &#x3D; \frac{correct}{Total}<br>$$</p>
<h3 id="2-3-3-Features"><a href="#2-3-3-Features" class="headerlink" title="2.3.3 Features"></a>2.3.3 Features</h3><ul>
<li>Features are patterns that are used to parameterise a model<ul>
<li>word : P(w)</li>
<li>n-gram : p(w<del>2</del>|w<del>1</del>)</li>
<li>word-class pair : P(w|c)</li>
</ul>
</li>
<li>with more features, we can obtain more evidences for making a correct prediction</li>
<li>overlapping features<ul>
<li>增量模型，重叠的特征不能同时出现在一个模型中</li>
</ul>
</li>
</ul>
<h1 id="Chapter-3"><a href="#Chapter-3" class="headerlink" title="Chapter 3"></a>Chapter 3</h1><h2 id="3-1-Representing-Documents-in-Vector-Spaces"><a href="#3-1-Representing-Documents-in-Vector-Spaces" class="headerlink" title="3.1 Representing Documents in Vector Spaces"></a>3.1 Representing Documents in Vector Spaces</h2><h4 id="Naive-Bayes-model"><a href="#Naive-Bayes-model" class="headerlink" title="Naive Bayes model"></a>Naive Bayes model</h4><p><img src="/2022/10/04/NLP-Westlake/image-20221001235301852.png" alt="image-20221001235301852"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221001235725478.png" alt="image-20221001235725478"></p>
<p>f 表示单词在文章d中出现的次数</p>
<h4 id="stop-words"><a href="#stop-words" class="headerlink" title="stop words"></a><strong>stop</strong> <strong>words</strong></h4><p>经常出现的 不能提供信息的词</p>
<p>a，the，on，of</p>
<p>从词表中删除（需要手动标注）</p>
<h4 id="TF-IDF-vector"><a href="#TF-IDF-vector" class="headerlink" title="TF-IDF vector"></a>TF-IDF vector</h4><ul>
<li>soft version of stop words in selecting useful words</li>
<li>the more documents in which of words exists, the less informative the word is</li>
<li>reduce the importance values of uninformative words</li>
</ul>
<p><img src="/2022/10/04/NLP-Westlake/image-20221002004047241.png" alt="image-20221002004047241"></p>
<p><strong>TF</strong> : w出现在文档里的次数 &#x2F;  文档里词的总数</p>
<p><strong>DF</strong>: 所有文档里出现w的文档数  &#x2F; 总文档数</p>
<p><strong>IDF</strong>: log  1&#x2F;DF</p>
<h3 id="3-1-1-Clustering"><a href="#3-1-1-Clustering" class="headerlink" title="3.1.1 Clustering"></a>3.1.1 Clustering</h3><p>​	To find groups of vectors that stay relatively close to each other,</p>
<p>using measures of distance in vector space </p>
<h4 id="向量之间距离计算方法"><a href="#向量之间距离计算方法" class="headerlink" title="向量之间距离计算方法"></a><strong>向量之间距离计算方法</strong></h4><ul>
<li><p>Euclidean distance 欧式距离</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221002124356663.png" alt="image-20221002124356663"></p>
</li>
<li><p>Cosine distance 夹角距离</p>
</li>
</ul>
<p><img src="/2022/10/04/NLP-Westlake/image-20221002124553512.png" alt="image-20221002124553512"></p>
<h3 id="3-1-2-K-means-clustering"><a href="#3-1-2-K-means-clustering" class="headerlink" title="3.1.2 K-means clustering"></a>3.1.2 K-means clustering</h3><p>Initialization: pre-specify the number of cluster k</p>
<p>​						random select k points as cluster centroids</p>
<p>Steps:</p>
<p>​		repeat:</p>
<p>​				a : assign each point to the cluster whose centroid is the closest;</p>
<p>​				b : reassign cluster centroids (by averaging points in each cluster)</p>
<p>​		until:</p>
<p>​				the cluster contents stablize</p>
<h3 id="3-1-3-Classification"><a href="#3-1-3-Classification" class="headerlink" title="3.1.3 Classification"></a>3.1.3 Classification</h3><p><strong>Clustering</strong> </p>
<ul>
<li>无监督学习 不需要人工标注训练数据</li>
<li>所有的单词重要性相同</li>
<li>无法进行细粒度的划分</li>
</ul>
<p><strong>Classification</strong></p>
<ul>
<li>有监督的学习 需要有实现人工标注好的训练集</li>
<li>可以选出重要的数据</li>
<li>使用特定的模型参数进行空间划分</li>
</ul>
<h3 id="3-1-4-SVM"><a href="#3-1-4-SVM" class="headerlink" title="3.1.4 SVM"></a>3.1.4 SVM</h3><p>support vector machine  支持向量机</p>
<p><strong>Hyperline</strong> 超平面</p>
<ul>
<li>高维向量空间中的线性空间</li>
<li>2-d : line</li>
<li>3-d : plane</li>
</ul>
<p><strong>Linear</strong> <strong>models</strong> ： 线性分类器</p>
<ul>
<li>使用超平面来分割数据的</li>
</ul>
<p><img src="/2022/10/04/NLP-Westlake/image-20221002144729586.png" alt="image-20221002144729586"></p>
<ul>
<li><h5 id="Support-vector"><a href="#Support-vector" class="headerlink" title="Support vector"></a><strong>Support vector</strong></h5><ul>
<li>距离切分的超平面最近的向量</li>
</ul>
</li>
<li><p><strong>Margins</strong></p>
</li>
<li><p>支持向量和超平面的距离</p>
</li>
<li><p><strong>Training goal</strong></p>
<ul>
<li>找到使Margins最大的超平面</li>
</ul>
</li>
</ul>
<h4 id="SVM-Classifier"><a href="#SVM-Classifier" class="headerlink" title="SVM Classifier"></a><strong>SVM Classifier</strong></h4><p>超平面定义:</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221002145405032.png" alt="image-20221002145405032"></p>
<p>w 是和超平面垂直的一个向量</p>
<p>b 是一个标量</p>
<ul>
<li>在超平面的两边，分别小于0，大于0</li>
<li>假设正例 y&#x3D;1，负例 y&#x3D;-1</li>
<li>向量到超平面的距离<ul>
<li><img src="/2022/10/04/NLP-Westlake/image-20221002145603687.png" alt="		"></li>
</ul>
</li>
</ul>
<p>目标 : 找到一个w和b，使得 r 最大 </p>
<p>由于 要求分母部分 &gt;&#x3D; 1 所有只要考虑 arg min || w ||</p>
<h3 id="3-1-5-Perceptron-感知机"><a href="#3-1-5-Perceptron-感知机" class="headerlink" title="3.1.5 Perceptron 感知机"></a>3.1.5 Perceptron 感知机</h3><p>The perceptron algorithm</p>
<ul>
<li>a liner model to find a value for (w,b) <ul>
<li>y &#x3D; SIGN( w^T^v(x) + b)</li>
</ul>
</li>
</ul>
<p><img src="/2022/10/04/NLP-Westlake/image-20221002180355379.png" alt="image-20221002180355379"></p>
<p>​			</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221002181634799.png" alt="image-20221002181634799"></p>
<p><strong>支持向量机和感知机</strong></p>
<p>共同点: 在向量空间对二分类问题进行了建模和参数化</p>
<p><strong>Batch-learning</strong>：批学习</p>
<p>SVM定义了完整的损失函数，通过优化损失函数来得到模型参数</p>
<p><strong>Online-learning</strong> : 在线学习</p>
<p>感知机直接给出训练算法，通过数据集中每一个样本的迭代得到参数。</p>
<h2 id="3-2-Multi-class-Classification"><a href="#3-2-Multi-class-Classification" class="headerlink" title="3.2 Multi-class Classification"></a>3.2 Multi-class Classification</h2><h3 id="3-2-1-Defining-output-based-features"><a href="#3-2-1-Defining-output-based-features" class="headerlink" title="3.2.1 Defining output-based features"></a>3.2.1 Defining output-based features</h3><p>基于输出的向量表示</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221002193639504.png" alt="image-20221002193639504"></p>
<p>将输出也作为特征加入到向量中</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221002193851332.png" alt="image-20221002193851332"></p>
<h3 id="3-2-2-Multi-class-SVM"><a href="#3-2-2-Multi-class-SVM" class="headerlink" title="3.2.2 Multi-class SVM"></a>3.2.2 Multi-class SVM</h3><p><img src="/2022/10/04/NLP-Westlake/image-20221002194255807.png" alt="image-20221002194255807"></p>
<p><strong>基于打分的多分类感知机</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221002194828864.png" alt="image-20221002194828864"></p>
<p>正样本和负样本的差值 &gt;&#x3D; 1</p>
<h3 id="3-3-3-Multi-class-perceptron"><a href="#3-3-3-Multi-class-perceptron" class="headerlink" title="3.3.3 Multi-class perceptron"></a>3.3.3 Multi-class perceptron</h3><p><img src="/2022/10/04/NLP-Westlake/image-20221002195201349.png" alt="image-20221002195201349"></p>
<p>加上正确的特征向量 v(xi,ci) ，减去错误的特征向量 v(xi,zi)</p>
<p><strong>证明过程:</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221002195444476.png" alt="image-20221002195444476"></p>
<h2 id="3-3-Discriminative-Models-and-Features"><a href="#3-3-Discriminative-Models-and-Features" class="headerlink" title="3.3 Discriminative Models and Features"></a>3.3 Discriminative Models and Features</h2><p><strong>判别式模型</strong></p>
<ul>
<li><p>SVM 和 感知机 是判别式模型</p>
<p>让正例得到的分数比负例的分数高，直接对正例和负例进行区分。</p>
</li>
<li><p>朴素贝叶斯模型是生成模型，计算输入与输出的共同概率</p>
</li>
<li><p>三者都是线性模型</p>
</li>
</ul>
<p><img src="/2022/10/04/NLP-Westlake/image-20221003123042744.png" alt="image-20221003123042744"></p>
<p><strong>判别式模型优点</strong> :  可以加入重叠特征</p>
<h4 id="Feature-templates-and-instances"><a href="#Feature-templates-and-instances" class="headerlink" title="Feature templates and instances"></a><strong>Feature templates and instances</strong></h4><p><img src="/2022/10/04/NLP-Westlake/image-20221003132424988.png" alt="image-20221003132424988"></p>
<h3 id="3-3-2-Dot-product-Form-of-Linear-Models"><a href="#3-3-2-Dot-product-Form-of-Linear-Models" class="headerlink" title="3.3.2 Dot-product Form of Linear Models"></a>3.3.2 Dot-product Form of Linear Models</h3><p><strong>线性模型的统一表达模式</strong></p>
<ul>
<li>Given an input x，its score</li>
</ul>
<p>  <img src="/2022/10/04/NLP-Westlake/image-20221003132808497.png" alt="image-20221003132808497"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221003132833897.png" alt="image-20221003132833897"></p>
<h3 id="3-3-3-Separability-and-Generalizability"><a href="#3-3-3-Separability-and-Generalizability" class="headerlink" title="3.3.3 Separability and Generalizability"></a>3.3.3 Separability and Generalizability</h3><h5 id="可分性和泛化性"><a href="#可分性和泛化性" class="headerlink" title="可分性和泛化性"></a><strong>可分性和泛化性</strong></h5><ul>
<li><strong>特征工程</strong> : 定义一个有用的特征集合<ul>
<li>特征越多 信息越多</li>
<li>better designed 特征工程有更好的可分性</li>
</ul>
</li>
<li><strong>Separability 可分性</strong><ul>
<li>线性可分</li>
</ul>
</li>
<li><strong>Generalization 泛化性</strong><ul>
<li>overfitting</li>
<li>underfittting</li>
</ul>
</li>
</ul>
<h5 id="线性不可分问题"><a href="#线性不可分问题" class="headerlink" title="线性不可分问题"></a>线性不可分问题</h5><p>设置一个松弛变量</p>
<p>最小化松弛变量</p>
<h5 id="二分类线性不可分SVM问题"><a href="#二分类线性不可分SVM问题" class="headerlink" title="二分类线性不可分SVM问题"></a>二分类线性不可分SVM问题</h5><p><img src="/2022/10/04/NLP-Westlake/image-20221003135909337.png" alt="image-20221003135909337"></p>
<p>C 为超参</p>
<h5 id="多分类线性不可分SVM问题"><a href="#多分类线性不可分SVM问题" class="headerlink" title="多分类线性不可分SVM问题"></a>多分类线性不可分SVM问题</h5><p><img src="/2022/10/04/NLP-Westlake/image-20221003140320704.png" alt="image-20221003140320704"></p>
<h5 id="感知机线性不可分"><a href="#感知机线性不可分" class="headerlink" title="感知机线性不可分"></a>感知机线性不可分</h5><p>仍然可用</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221003140555278.png" alt="image-20221003140555278"></p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p><img src="/2022/10/04/NLP-Westlake/image-20221003140745816.png" alt="image-20221003140745816"></p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/10/04/ML-%E5%90%B4%E6%81%A9%E8%BE%BE/" rel="prev" title="Machine Learning   --wuenda">
      <i class="fa fa-chevron-left"></i> Machine Learning   --wuenda
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/10/04/%E5%B7%A5%E4%BD%9C%E6%97%A5%E5%BF%97/" rel="next" title="工作日志">
      工作日志 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#chapter-1"><span class="nav-number">1.</span> <span class="nav-text">chapter 1</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-what-is-nlp"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 what is  nlp</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E8%87%AA%E5%8A%A8%E7%9A%84%E7%90%86%E8%A7%A3%E5%92%8C%E7%94%9F%E6%88%90%E4%BA%BA%E7%B1%BB%E8%AF%AD%E8%A8%80"><span class="nav-number">1.1.0.0.0.1.</span> <span class="nav-text">自动的理解和生成人类语言</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Main-approaches"><span class="nav-number">1.1.1.</span> <span class="nav-text">Main approaches</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Rule-based-symbolic-approach-%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">1.1.1.0.0.1.</span> <span class="nav-text">Rule-based(symbolic) approach  基于规则的方法</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-NLP-tasks"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 NLP  tasks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-1-Fundamental-Tasks"><span class="nav-number">1.2.1.</span> <span class="nav-text">1.2.1 Fundamental Tasks</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Syntactic-tasks-%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90%E4%BB%BB%E5%8A%A1"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">Syntactic tasks 句法分析任务</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Semantic-task-%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90%E4%BB%BB%E5%8A%A1"><span class="nav-number">1.2.1.2.</span> <span class="nav-text">Semantic task 语义分析任务</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Discourse-tasks-%E7%AF%87%E7%AB%A0%E5%88%86%E6%9E%90"><span class="nav-number">1.2.1.3.</span> <span class="nav-text">Discourse tasks 篇章分析</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-2-Information-extraction-%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96"><span class="nav-number">1.2.2.</span> <span class="nav-text">1.2.2 Information extraction 信息抽取</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Entities"><span class="nav-number">1.2.2.0.0.1.</span> <span class="nav-text">Entities</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-3-Text-Generation-Tasks"><span class="nav-number">1.2.3.</span> <span class="nav-text">1.2.3 Text Generation Tasks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-4-other-applications"><span class="nav-number">1.2.4.</span> <span class="nav-text">1.2.4 other applications</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-NLP-from-a-Machine-Learning-Perspective"><span class="nav-number">1.3.</span> <span class="nav-text">1.3 NLP from a Machine Learning Perspective</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Chapter-2"><span class="nav-number">2.</span> <span class="nav-text">Chapter 2</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-what-is-model"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 what is model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-1-MLE"><span class="nav-number">2.1.1.</span> <span class="nav-text">2.1.1 MLE</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-N-gram-Language-Models"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 N-gram Language Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-2-Bigram-Language-Models"><span class="nav-number">2.2.1.</span> <span class="nav-text">2.2.2 Bigram Language Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-3-Trigram-Language-Models-and-Beyond"><span class="nav-number">2.2.2.</span> <span class="nav-text">2.2.3 Trigram Language Models and Beyond</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Methods-to-address-sparsity"><span class="nav-number">2.2.3.</span> <span class="nav-text">Methods to address sparsity</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Log-probability-models"><span class="nav-number">2.2.4.</span> <span class="nav-text">Log-probability models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-4-Generative-Models"><span class="nav-number">2.2.5.</span> <span class="nav-text">2.2.4 Generative Models</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-Naive-Bayes-rule"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 Naive Bayes rule</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-2-Evaluating-a-Text-Classifier"><span class="nav-number">2.3.1.</span> <span class="nav-text">2.3.2 Evaluating a Text Classifier</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-3-Features"><span class="nav-number">2.3.2.</span> <span class="nav-text">2.3.3 Features</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Chapter-3"><span class="nav-number">3.</span> <span class="nav-text">Chapter 3</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-Representing-Documents-in-Vector-Spaces"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 Representing Documents in Vector Spaces</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Naive-Bayes-model"><span class="nav-number">3.1.0.1.</span> <span class="nav-text">Naive Bayes model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#stop-words"><span class="nav-number">3.1.0.2.</span> <span class="nav-text">stop words</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#TF-IDF-vector"><span class="nav-number">3.1.0.3.</span> <span class="nav-text">TF-IDF vector</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-1-Clustering"><span class="nav-number">3.1.1.</span> <span class="nav-text">3.1.1 Clustering</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%91%E9%87%8F%E4%B9%8B%E9%97%B4%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95"><span class="nav-number">3.1.1.1.</span> <span class="nav-text">向量之间距离计算方法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-2-K-means-clustering"><span class="nav-number">3.1.2.</span> <span class="nav-text">3.1.2 K-means clustering</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-3-Classification"><span class="nav-number">3.1.3.</span> <span class="nav-text">3.1.3 Classification</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-4-SVM"><span class="nav-number">3.1.4.</span> <span class="nav-text">3.1.4 SVM</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Support-vector"><span class="nav-number">3.1.4.0.1.</span> <span class="nav-text">Support vector</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SVM-Classifier"><span class="nav-number">3.1.4.1.</span> <span class="nav-text">SVM Classifier</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-5-Perceptron-%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="nav-number">3.1.5.</span> <span class="nav-text">3.1.5 Perceptron 感知机</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-Multi-class-Classification"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 Multi-class Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-1-Defining-output-based-features"><span class="nav-number">3.2.1.</span> <span class="nav-text">3.2.1 Defining output-based features</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-2-Multi-class-SVM"><span class="nav-number">3.2.2.</span> <span class="nav-text">3.2.2 Multi-class SVM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-3-Multi-class-perceptron"><span class="nav-number">3.2.3.</span> <span class="nav-text">3.3.3 Multi-class perceptron</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-Discriminative-Models-and-Features"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 Discriminative Models and Features</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Feature-templates-and-instances"><span class="nav-number">3.3.0.1.</span> <span class="nav-text">Feature templates and instances</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-2-Dot-product-Form-of-Linear-Models"><span class="nav-number">3.3.1.</span> <span class="nav-text">3.3.2 Dot-product Form of Linear Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-3-Separability-and-Generalizability"><span class="nav-number">3.3.2.</span> <span class="nav-text">3.3.3 Separability and Generalizability</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8F%AF%E5%88%86%E6%80%A7%E5%92%8C%E6%B3%9B%E5%8C%96%E6%80%A7"><span class="nav-number">3.3.2.0.1.</span> <span class="nav-text">可分性和泛化性</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E4%B8%8D%E5%8F%AF%E5%88%86%E9%97%AE%E9%A2%98"><span class="nav-number">3.3.2.0.2.</span> <span class="nav-text">线性不可分问题</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BA%8C%E5%88%86%E7%B1%BB%E7%BA%BF%E6%80%A7%E4%B8%8D%E5%8F%AF%E5%88%86SVM%E9%97%AE%E9%A2%98"><span class="nav-number">3.3.2.0.3.</span> <span class="nav-text">二分类线性不可分SVM问题</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%A4%9A%E5%88%86%E7%B1%BB%E7%BA%BF%E6%80%A7%E4%B8%8D%E5%8F%AF%E5%88%86SVM%E9%97%AE%E9%A2%98"><span class="nav-number">3.3.2.0.4.</span> <span class="nav-text">多分类线性不可分SVM问题</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%BA%BF%E6%80%A7%E4%B8%8D%E5%8F%AF%E5%88%86"><span class="nav-number">3.3.2.0.5.</span> <span class="nav-text">感知机线性不可分</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Summary"><span class="nav-number">3.4.</span> <span class="nav-text">Summary</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Moriarty</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">9</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Moriarty</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
