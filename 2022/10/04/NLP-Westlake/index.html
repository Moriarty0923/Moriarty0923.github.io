<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"moriarty0923.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="NLP —— Westlake 西湖大学张岳NLP教程">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP ---- Westlake">
<meta property="og:url" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/index.html">
<meta property="og:site_name" content="Moriarty&#39; blog">
<meta property="og:description" content="NLP —— Westlake 西湖大学张岳NLP教程">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20220924152854633-1666628527049-1.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20220926225354087-1666628527049-2.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20220926225757505-1666628527049-3.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20220927230300022-1666628527049-4.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20220927233250125-1666628527049-5.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20220927233747510-1666628527049-6.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20220927234053599-1666628527049-7.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20220929212906928-1666628527049-8.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20220929214219674-1666628527049-9.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20220929214618814-1666628527049-10.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20220929214904593-1666628527049-11.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20220929233540029-1666628527049-13.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20220930234129959-1666628527049-12.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221001235301852-1666628527050-14.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221001235725478-1666628527050-15.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221002004047241-1666628527050-16.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221002124356663-1666628527050-17.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221002124553512-1666628527050-18.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221002144729586-1666628527050-19.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221002145405032-1666628527050-20.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221002145603687-1666628527050-21.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221002180355379-1666628527050-22.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221002181634799-1666628527050-23.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221002193639504-1666628527050-24.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221002193851332-1666628527050-25.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221002194255807-1666628527050-26.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221002194828864-1666628527050-27.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221002195201349-1666628527050-28.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221002195444476-1666628527050-29.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221003123042744-1666628527050-30.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221003132424988-1666628527050-32.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221003132808497-1666628527050-31.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221003132833897-1666628527050-34.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221003135909337-1666628527050-33.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221003140320704-1666628527050-35.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221003140555278-1666628527050-36.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221003140745816-1666628527050-37.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221004163416870-1666628527050-38.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221004165706198-1666628527050-39.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221004170308712-1666628527050-40.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221004184313222-1666628527051-47.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221004184710100-1666628527050-41.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221004202938705-1666628527050-42.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221004204134439-1666628527050-43.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221004204540158-1666628527050-44.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/Desktop/file/ywj/Aispeech-ywj/%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/%E7%AC%94%E8%AE%B0/NLP-Westlake/image-20221004205102711.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221004213356396-1666628527051-45.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221004213826312-1666628527051-49.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/Desktop/file/ywj/Aispeech-ywj/%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/%E7%AC%94%E8%AE%B0/NLP-Westlake/image-20221004214007325.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221004214227743-1666628527051-46.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221004220922092-1666628527051-48.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221004221711899.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221004225041037-1666628527051-50.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221004225800479-1666628527051-51.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221004230228385-1666628527051-52.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221004231033840-1666628527051-53.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221004231420718.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221006150341327-1666628527051-54.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221006150404262-1666628527051-55.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221006150814514-1666628527051-56.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221006150940811-1666628527051-57.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221006150953698-1666628527051-58.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221006151320353-1666628527051-59.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221006151846609-1666628527051-60.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221006163655149.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221006163931533-1666628527051-61.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221006164225289-1666628527051-62.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221007230900867-1666628527051-63.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221007231053730-1666628527051-64.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221007232603784-1666628527051-65.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221007232632864-1666628527051-66.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221007233139371-1666628527051-67.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221015212207641-1666628527051-68.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221017121821430-1666628527051-69.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221017130050466-1666628527051-70.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221017130546965-1666628527051-71.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221017130727310-1666628527051-72.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221017131312477-1666628527051-73.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221017131433443-1666628527051-74.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221017131617762-1666628527051-75.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221017131744720-1666628527051-76.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221017131847932-1666628527051-77.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221017150504790-1666628527051-78.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221017153622830-1666628527051-79.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221017153822650-1666628527051-80.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221017154444202-1666628527051-81.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221017164713863-1666628527051-82.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221017170306833-1666628527051-83.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221017170713916-1666628527051-84.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221017170735047-1666628527051-85.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221017171048743-1666628527051-86.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221018201827997-1666628527051-87.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221018202110954-1666628527051-88.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221018202454631-1666628527051-89.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221018202609727-1666628527051-90.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221018202957428-1666628527051-95.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221018210517330-1666628527051-91.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221018211534302-1666628527051-92.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221018211624575-1666628527051-93.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221018221847991-1666628527051-94.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221018222326525-1666628527052-96.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221018222859775-1666628527052-97.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221018223244443-1666628527052-98.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221018223417466-1666628527052-99.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221018223453208-1666628527052-100.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221018224008883-1666628527052-101.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221018224113586-1666628527052-102.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221019003733876-1666628527052-103.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221019014134737-1666628527052-104.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221019014845714-1666628527052-105.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221021000622639-1666628527052-106.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221021001014101-1666628527052-107.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221021001122812-1666628527052-108.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221021001813414-1666628527052-109.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221021001958934-1666628527052-110.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221021002007628-1666628527052-111.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221021003435297-1666628527052-112.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221021004131041-1666628527052-113.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221021005805885-1666628527052-114.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221021010419619-1666628527052-115.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221021010433326-1666628527052-116.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221021010618556-1666628527052-117.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221021010745999-1666628527052-118.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221021231732220-1666628527052-119.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221021231925535-1666628527052-120.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221021232415167-1666628527052-121.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221021232627619-1666628527052-122.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221021233223004-1666628527052-123.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221022000341229-1666628527052-124.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221022000545562-1666628527052-125.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221022000740607-1666628527052-126.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221022001304474-1666628527052-127.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221022001827461-1666628527052-128.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221022005248436-1666628527052-129.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221022005554840-1666628527052-130.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221022005838839-1666628527052-131.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221022010344493-1666628527052-132.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221022010640690-1666628527052-133.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221022010849015-1666628527052-134.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221022010923208-1666628527052-135.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221022011308997-1666628527052-136.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221022011343074-1666628527052-137.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221022011500750-1666628527052-138.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221022011627093-1666628527052-139.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221022011716811-1666628527052-140.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221022011737724-1666628527052-141.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221022012145804-1666628527052-142.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221022225509854-1666628527052-143.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221022225814160-1666628527052-144.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221022230503931-1666628527052-145.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221022230607295-1666628527052-146.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221023004746272-1666628527052-147.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221023005231211-1666628527052-148.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221023005345148-1666628527052-149.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221023005455727-1666628527052-150.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221023005518712-1666628527052-151.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221023005605196-1666628527052-152.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221023005614961-1666628527052-153.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221023014328397-1666628527053-154.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221023014705862-1666628527053-155.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221023015016969-1666628527053-156.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221023015225776-1666628527053-157.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221023015605985-1666628527053-158.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221023015652160-1666628527053-159.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221023015932606-1666628527053-160.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221025001200021-1666628527053-161.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221025001359083-1666628527053-162.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221025001603936-1666628527053-163.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221025001911763-1666628527053-164.png">
<meta property="og:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20221025003528211.png">
<meta property="article:published_time" content="2022-10-03T16:23:08.156Z">
<meta property="article:modified_time" content="2022-11-04T13:24:55.192Z">
<meta property="article:author" content="Moriarty">
<meta property="article:tag" content="笔记">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/image-20220924152854633-1666628527049-1.png">

<link rel="canonical" href="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>NLP ---- Westlake | Moriarty' blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Moriarty' blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-books">

    <a href="/books/" rel="section"><i class="fa fa-calendar fa-fw"></i>books</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://moriarty0923.github.io/2022/10/04/NLP-Westlake/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Moriarty">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Moriarty' blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          NLP ---- Westlake
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-10-04 00:23:08" itemprop="dateCreated datePublished" datetime="2022-10-04T00:23:08+08:00">2022-10-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-11-04 21:24:55" itemprop="dateModified" datetime="2022-11-04T21:24:55+08:00">2022-11-04</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><strong>NLP —— Westlake</strong></p>
<p><strong>西湖大学张岳NLP教程</strong></p>
<span id="more"></span>

<h1 id="chapter-1"><a href="#chapter-1" class="headerlink" title="chapter 1"></a><strong>chapter 1</strong></h1><h2 id="1-1-what-is-nlp"><a href="#1-1-what-is-nlp" class="headerlink" title="1.1 what is  nlp"></a>1.1 what is  nlp</h2><ul>
<li><h6 id="自动的理解和生成人类语言"><a href="#自动的理解和生成人类语言" class="headerlink" title="自动的理解和生成人类语言"></a>自动的理解和生成人类语言</h6></li>
</ul>
<h3 id="Main-approaches"><a href="#Main-approaches" class="headerlink" title="Main approaches"></a>Main approaches</h3><ul>
<li><h6 id="Rule-based-symbolic-approach-基于规则的方法"><a href="#Rule-based-symbolic-approach-基于规则的方法" class="headerlink" title="Rule-based(symbolic) approach  基于规则的方法"></a>Rule-based(symbolic) approach  基于规则的方法</h6></li>
<li><p><strong>Statistical approach(traditional machine learning) 基于统计的方法</strong></p>
</li>
<li><p><strong>Connectionist approach(Neural networks) 神经网络</strong></p>
</li>
</ul>
<h2 id="1-2-NLP-tasks"><a href="#1-2-NLP-tasks" class="headerlink" title="1.2 NLP  tasks"></a>1.2 NLP  tasks</h2><h3 id="1-2-1-Fundamental-Tasks"><a href="#1-2-1-Fundamental-Tasks" class="headerlink" title="1.2.1 Fundamental Tasks"></a>1.2.1 Fundamental Tasks</h3><h4 id="Syntactic-tasks-句法分析任务"><a href="#Syntactic-tasks-句法分析任务" class="headerlink" title="Syntactic tasks 句法分析任务"></a>Syntactic tasks 句法分析任务</h4><ul>
<li><strong>word level</strong>  <ul>
<li>Morphological analysis 形态化分析 <ul>
<li>词根和词缀提取分析 walking  –&gt; walk + ing</li>
</ul>
</li>
<li>word segmentation 分词（中文）<ul>
<li>字的序列分割成词的序列</li>
</ul>
</li>
<li>tokenization 分词 （英文）</li>
<li>POS  tagging 词性标注</li>
</ul>
</li>
<li><strong>sentence level 句子结构</strong><ul>
<li>Constituent parsing 成分句法 (层次短语句法)<ul>
<li>通过层次化的短语结构来表达一句话</li>
</ul>
</li>
<li>Dependency parsing 依存句法<ul>
<li>通过两个词的关系来组成结构</li>
<li>每个词修饰句中唯一一个词</li>
<li>有一次词不修饰任何词，root节点</li>
<li>树结构</li>
</ul>
</li>
<li>CCG parsing 组合范畴句法<ul>
<li>高度词汇化的句法</li>
<li>每一个词都有一个复杂的词汇化标签</li>
<li>CCG supertagging</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Semantic-task-语义分析任务"><a href="#Semantic-task-语义分析任务" class="headerlink" title="Semantic task 语义分析任务"></a>Semantic task 语义分析任务</h4><ul>
<li><strong>word level</strong>  <ul>
<li>Word sense disambiguation (WSD) 语义消歧</li>
<li>Metaphor 隐喻检测</li>
<li>Sense relations between words 词之间的关系</li>
</ul>
</li>
<li><strong>sentence level</strong><ul>
<li>Predicate-argument relations 谓词论元结构</li>
</ul>
</li>
</ul>
<h4 id="Discourse-tasks-篇章分析"><a href="#Discourse-tasks-篇章分析" class="headerlink" title="Discourse tasks 篇章分析"></a>Discourse tasks <strong>篇章分析</strong></h4><ul>
<li>discourse segmentation 篇章切分</li>
</ul>
<h3 id="1-2-2-Information-extraction-信息抽取"><a href="#1-2-2-Information-extraction-信息抽取" class="headerlink" title="1.2.2 Information extraction 信息抽取"></a>1.2.2 Information extraction 信息抽取</h3><ul>
<li><h6 id="Entities"><a href="#Entities" class="headerlink" title="Entities"></a><strong>Entities</strong></h6><ul>
<li>Named entity recognition 命名实体识别</li>
<li>Anaphora 指代消解</li>
<li>co-references 共指消解</li>
</ul>
</li>
<li><p><strong>Relations</strong></p>
<ul>
<li>relation extraction</li>
</ul>
</li>
<li><p><strong>Knowlege</strong> <strong>graph</strong> <strong>知识图谱</strong></p>
<p>a type of db , entities form nodes and relations form edges</p>
<ul>
<li>entity linking 实体链接</li>
<li>named entity normalization 命名实体规范化</li>
<li>link prediction 链接预测</li>
</ul>
</li>
<li><p><strong>Events</strong></p>
<ul>
<li>News event detection</li>
<li>Event factuality prediction 事件真实性检测</li>
<li>Event time extration 时间线检测</li>
<li>causality detection 事件因果关系</li>
<li>scipt learning 脚本学习</li>
</ul>
</li>
<li><p><strong>Sentiment</strong> <strong>analysis</strong></p>
<ul>
<li>Sentiment classification 情感分类</li>
<li>Targeted sentiment 基于对象的情感</li>
<li>Aspect-oriented sentiment 基于方面的情感</li>
<li>More fine-grained sentiment classification 更复杂的序列化情感</li>
<li>Sentiment detection</li>
<li>sentiment lexicon acquisition</li>
<li>emotion detection</li>
<li>stance detection and argumentation mining 立场检测</li>
</ul>
</li>
</ul>
<h3 id="1-2-3-Text-Generation-Tasks"><a href="#1-2-3-Text-Generation-Tasks" class="headerlink" title="1.2.3 Text Generation Tasks"></a>1.2.3 Text Generation Tasks</h3><ul>
<li><strong>Realization 实现</strong>   <ul>
<li>语义到文字的生成</li>
</ul>
</li>
<li><strong>Data-to-text Generation</strong></li>
<li><strong>Summarization</strong></li>
<li><strong>Machine</strong> <strong>translation</strong></li>
<li><strong>Grammar error correction</strong></li>
<li><strong>Question answering</strong></li>
<li><strong>Dialogue</strong> <strong>systems</strong></li>
</ul>
<h3 id="1-2-4-other-applications"><a href="#1-2-4-other-applications" class="headerlink" title="1.2.4 other applications"></a>1.2.4 other applications</h3><ul>
<li><strong>Information retrieval</strong> 信息检索</li>
<li><strong>Recommendation system</strong></li>
<li><strong>Text mining and text analytics</strong></li>
</ul>
<h2 id="1-3-NLP-from-a-Machine-Learning-Perspective"><a href="#1-3-NLP-from-a-Machine-Learning-Perspective" class="headerlink" title="1.3 NLP from a Machine Learning Perspective"></a>1.3 NLP from a Machine Learning Perspective</h2><p><img src="/2022/10/04/NLP-Westlake/image-20220924152854633-1666628527049-1.png" alt="image-20220924152854633"></p>
<ul>
<li>语言学特点</li>
<li>机器学习特征</li>
<li>数据特征</li>
</ul>
<p><strong>Machine</strong> <strong>learning perspective</strong></p>
<p>从机器学习的角度对NLP任务进行分类</p>
<ul>
<li><strong>Classification</strong> </li>
<li><strong>Structure</strong> <strong>prediction</strong> <strong>结构预测</strong></li>
<li><strong>Regression</strong></li>
</ul>
<p>从训练数据角度</p>
<ul>
<li>无监督学习</li>
<li>有监督学习</li>
<li>半监督学习</li>
</ul>
<h1 id="Chapter-2"><a href="#Chapter-2" class="headerlink" title="Chapter 2"></a><strong>Chapter</strong> <strong>2</strong></h1><h2 id="2-1-what-is-model"><a href="#2-1-what-is-model" class="headerlink" title="2.1 what is model"></a>2.1 what is model</h2><p><strong>Models</strong></p>
<p>​	是对一个特定任务或事件的抽象，使数学计算变得可行。</p>
<p><strong>Probabilistic</strong> <strong>model</strong>  概率模型</p>
<p>​	随机事件出现的可能性</p>
<h3 id="2-1-1-MLE"><a href="#2-1-1-MLE" class="headerlink" title="2.1.1 MLE"></a>2.1.1 MLE</h3><p>​	Maximum Likelihood  Estimation</p>
<p>​	最大似然估算</p>
<p> <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/148968222">https://zhuanlan.zhihu.com/p/148968222</a></p>
<h2 id="2-2-N-gram-Language-Models"><a href="#2-2-N-gram-Language-Models" class="headerlink" title="2.2 N-gram Language Models"></a>2.2 N-gram Language Models</h2><p><strong>n元语言模型</strong></p>
<p>语言模型 : 衡量一句自然语言的概率的模型</p>
<p>eg : P (“eat pizza” ) &gt; P ( “drink pizza” )</p>
<p><strong>N-gram</strong> </p>
<p>​	通过n元组来进行句子概率的计算</p>
<p>​	<strong>假定一个词出现的概率只与它前面固定数目的词相关</strong></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/114538417">https://zhuanlan.zhihu.com/p/114538417</a></p>
<p><strong>Unigram Language Models</strong>  </p>
<p>​	通过单个单词的概率计算句子的概率</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20220926225354087-1666628527049-2.png" alt="image-20220926225354087"></p>
<p><strong>OOV</strong> : out - of - vocabulary </p>
<ul>
<li>not seen in the training data</li>
<li>p(oov) &#x3D; 0</li>
<li>p(s) &#x3D;0  if oov in s</li>
</ul>
<p><strong>add-one smoothing</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20220926225757505-1666628527049-3.png" alt="image-20220926225757505"></p>
<p><strong>Hyper - parameter</strong> 超参</p>
<ul>
<li>fixed in advance and not trained during train</li>
<li>can be tuned , selected empirically to improve performance</li>
</ul>
<h3 id="2-2-2-Bigram-Language-Models"><a href="#2-2-2-Bigram-Language-Models" class="headerlink" title="2.2.2 Bigram Language Models"></a>2.2.2 Bigram Language Models</h3><p>二元语言模型</p>
<p> 一元语言模型无法解决：</p>
<p>“he ate pizza”   or   “he drank pizza”</p>
<p>二元语言模型计算条件概率 <strong>conditional probailities</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20220927230300022-1666628527049-4.png" alt="image-20220927230300022"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20220927233250125-1666628527049-5.png" alt="image-20220927233250125"></p>
<p><strong>sparsity</strong> : 稀疏性</p>
<p>测试集中的二元组没有在词表中出现过</p>
<p><strong>解决方法 ： Back-off</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20220927233747510-1666628527049-6.png" alt="image-20220927233747510"></p>
<p><strong>句子概率的计算</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20220927234053599-1666628527049-7.png" alt="image-20220927234053599"></p>
<h3 id="2-2-3-Trigram-Language-Models-and-Beyond"><a href="#2-2-3-Trigram-Language-Models-and-Beyond" class="headerlink" title="2.2.3 Trigram Language Models and Beyond"></a>2.2.3 Trigram Language Models and Beyond</h3><p><strong>假设概率只与前两个词有关</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20220929212906928-1666628527049-8.png" alt="image-20220929212906928"></p>
<h3 id="Methods-to-address-sparsity"><a href="#Methods-to-address-sparsity" class="headerlink" title="Methods to address sparsity"></a>Methods to address sparsity</h3><ul>
<li><p>add-one smoothing : add one to the count of all words </p>
</li>
<li><p>add-α smoothing : add α to the count of all words </p>
</li>
<li><p>back-off : use lower order n-gram probabilities to approximate high order n-gram probabilities</p>
</li>
<li><p>Good-Turing smoothing : make a rational guess of the count of OOV words</p>
</li>
<li><p>Knesser-Ney smoothing : work with back-off,consider the history context of lower order n-gram</p>
</li>
</ul>
<h3 id="Log-probability-models"><a href="#Log-probability-models" class="headerlink" title="Log-probability models"></a>Log-probability models</h3><p><img src="/2022/10/04/NLP-Westlake/image-20220929214219674-1666628527049-9.png" alt="image-20220929214219674"></p>
<p><strong>解决浮点数精度问题</strong></p>
<h3 id="2-2-4-Generative-Models"><a href="#2-2-4-Generative-Models" class="headerlink" title="2.2.4 Generative Models"></a>2.2.4 Generative Models</h3><p>生成模型 ： 把一句话里的每个词逐一的生成出来</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20220929214618814-1666628527049-10.png" alt="image-20220929214618814"></p>
<p>马尔可夫模型</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20220929214904593-1666628527049-11.png" alt="image-20220929214904593"></p>
<h2 id="2-3-Naive-Bayes-rule"><a href="#2-3-Naive-Bayes-rule" class="headerlink" title="2.3 Naive Bayes rule"></a>2.3 Naive Bayes rule</h2><p>We have </p>
<p>​	<strong>P(AB) &#x3D; P(A|B) P(B) &#x3D; P(B|A) P(A)</strong></p>
<p>Therefore</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20220929233540029-1666628527049-13.png" alt="image-20220929233540029"></p>
<p>–Bayes rule</p>
<h3 id="2-3-2-Evaluating-a-Text-Classifier"><a href="#2-3-2-Evaluating-a-Text-Classifier" class="headerlink" title="2.3.2 Evaluating a Text Classifier"></a>2.3.2 Evaluating a Text Classifier</h3><p><strong>Data:</strong></p>
<ul>
<li>Traing set </li>
<li>Test set</li>
<li>Development set  调整超参</li>
</ul>
<p><img src="/2022/10/04/NLP-Westlake/image-20220930234129959-1666628527049-12.png" alt="image-20220930234129959"></p>
<p>Evaluation  metric</p>
<ul>
<li>Accuracy</li>
</ul>
<p>$$<br>Acc &#x3D; \frac{correct}{Total}<br>$$</p>
<h3 id="2-3-3-Features"><a href="#2-3-3-Features" class="headerlink" title="2.3.3 Features"></a>2.3.3 Features</h3><ul>
<li>Features are patterns that are used to parameterise a model<ul>
<li>word : P(w)</li>
<li>n-gram : p(w<del>2</del>|w<del>1</del>)</li>
<li>word-class pair : P(w|c)</li>
</ul>
</li>
<li>with more features, we can obtain more evidences for making a correct prediction</li>
<li>overlapping features<ul>
<li>增量模型，重叠的特征不能同时出现在一个模型中</li>
</ul>
</li>
</ul>
<h1 id="Chapter-3"><a href="#Chapter-3" class="headerlink" title="Chapter 3"></a>Chapter 3</h1><h2 id="3-1-Representing-Documents-in-Vector-Spaces"><a href="#3-1-Representing-Documents-in-Vector-Spaces" class="headerlink" title="3.1 Representing Documents in Vector Spaces"></a>3.1 Representing Documents in Vector Spaces</h2><h4 id="Naive-Bayes-model"><a href="#Naive-Bayes-model" class="headerlink" title="Naive Bayes model"></a>Naive Bayes model</h4><p><img src="/2022/10/04/NLP-Westlake/image-20221001235301852-1666628527050-14.png" alt="image-20221001235301852"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221001235725478-1666628527050-15.png" alt="image-20221001235725478"></p>
<p>f 表示单词在文章d中出现的次数</p>
<h4 id="stop-words"><a href="#stop-words" class="headerlink" title="stop words"></a><strong>stop</strong> <strong>words</strong></h4><p>经常出现的 不能提供信息的词</p>
<p>a，the，on，of</p>
<p>从词表中删除（需要手动标注）</p>
<h4 id="TF-IDF-vector"><a href="#TF-IDF-vector" class="headerlink" title="TF-IDF vector"></a>TF-IDF vector</h4><ul>
<li>soft version of stop words in selecting useful words</li>
<li>the more documents in which of words exists, the less informative the word is</li>
<li>reduce the importance values of uninformative words</li>
</ul>
<p><img src="/2022/10/04/NLP-Westlake/image-20221002004047241-1666628527050-16.png" alt="image-20221002004047241"></p>
<p><strong>TF</strong> : w出现在文档里的次数 &#x2F;  文档里词的总数</p>
<p><strong>DF</strong>: 所有文档里出现w的文档数  &#x2F; 总文档数</p>
<p><strong>IDF</strong>: log  1&#x2F;DF</p>
<h3 id="3-1-1-Clustering"><a href="#3-1-1-Clustering" class="headerlink" title="3.1.1 Clustering"></a>3.1.1 Clustering</h3><p>​	To find groups of vectors that stay relatively close to each other,</p>
<p>using measures of distance in vector space </p>
<h4 id="向量之间距离计算方法"><a href="#向量之间距离计算方法" class="headerlink" title="向量之间距离计算方法"></a><strong>向量之间距离计算方法</strong></h4><ul>
<li><p>Euclidean distance 欧式距离</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221002124356663-1666628527050-17.png" alt="image-20221002124356663"></p>
</li>
<li><p>Cosine distance 夹角距离</p>
</li>
</ul>
<p><img src="/2022/10/04/NLP-Westlake/image-20221002124553512-1666628527050-18.png" alt="image-20221002124553512"></p>
<h3 id="3-1-2-K-means-clustering"><a href="#3-1-2-K-means-clustering" class="headerlink" title="3.1.2 K-means clustering"></a>3.1.2 K-means clustering</h3><p>Initialization: pre-specify the number of cluster k</p>
<p>​						random select k points as cluster centroids</p>
<p>Steps:</p>
<p>​		repeat:</p>
<p>​				a : assign each point to the cluster whose centroid is the closest;</p>
<p>​				b : reassign cluster centroids (by averaging points in each cluster)</p>
<p>​		until:</p>
<p>​				the cluster contents stablize</p>
<h3 id="3-1-3-Classification"><a href="#3-1-3-Classification" class="headerlink" title="3.1.3 Classification"></a>3.1.3 Classification</h3><p><strong>Clustering</strong> </p>
<ul>
<li>无监督学习 不需要人工标注训练数据</li>
<li>所有的单词重要性相同</li>
<li>无法进行细粒度的划分</li>
</ul>
<p><strong>Classification</strong></p>
<ul>
<li>有监督的学习 需要有实现人工标注好的训练集</li>
<li>可以选出重要的数据</li>
<li>使用特定的模型参数进行空间划分</li>
</ul>
<h3 id="3-1-4-SVM"><a href="#3-1-4-SVM" class="headerlink" title="3.1.4 SVM"></a>3.1.4 SVM</h3><p>support vector machine  支持向量机</p>
<p><strong>Hyperline</strong> 超平面</p>
<ul>
<li>高维向量空间中的线性空间</li>
<li>2-d : line</li>
<li>3-d : plane</li>
</ul>
<p><strong>Linear</strong> <strong>models</strong> ： 线性分类器</p>
<ul>
<li>使用超平面来分割数据的</li>
</ul>
<p><img src="/2022/10/04/NLP-Westlake/image-20221002144729586-1666628527050-19.png" alt="image-20221002144729586"></p>
<ul>
<li><h5 id="Support-vector"><a href="#Support-vector" class="headerlink" title="Support vector"></a><strong>Support vector</strong></h5><ul>
<li>距离切分的超平面最近的向量</li>
</ul>
</li>
<li><p><strong>Margins</strong></p>
</li>
<li><p>支持向量和超平面的距离</p>
</li>
<li><p><strong>Training goal</strong></p>
<ul>
<li>找到使Margins最大的超平面</li>
</ul>
</li>
</ul>
<h4 id="SVM-Classifier"><a href="#SVM-Classifier" class="headerlink" title="SVM Classifier"></a><strong>SVM Classifier</strong></h4><p>超平面定义:</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221002145405032-1666628527050-20.png" alt="image-20221002145405032"></p>
<p>w 是和超平面垂直的一个向量</p>
<p>b 是一个标量</p>
<ul>
<li>在超平面的两边，分别小于0，大于0</li>
<li>假设正例 y&#x3D;1，负例 y&#x3D;-1</li>
<li>向量到超平面的距离<ul>
<li><img src="/2022/10/04/NLP-Westlake/image-20221002145603687-1666628527050-21.png" alt="		"></li>
</ul>
</li>
</ul>
<p>目标 : 找到一个w和b，使得 r 最大 </p>
<p>由于 要求分母部分 &gt;&#x3D; 1 所有只要考虑 arg min || w ||</p>
<h3 id="3-1-5-Perceptron-感知机"><a href="#3-1-5-Perceptron-感知机" class="headerlink" title="3.1.5 Perceptron 感知机"></a>3.1.5 Perceptron 感知机</h3><p>The perceptron algorithm</p>
<ul>
<li>a liner model to find a value for (w,b) <ul>
<li>y &#x3D; SIGN( w^T^v(x) + b)</li>
</ul>
</li>
</ul>
<p><img src="/2022/10/04/NLP-Westlake/image-20221002180355379-1666628527050-22.png" alt="image-20221002180355379"></p>
<p>​			</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221002181634799-1666628527050-23.png" alt="image-20221002181634799"></p>
<p><strong>支持向量机和感知机</strong></p>
<p>共同点: 在向量空间对二分类问题进行了建模和参数化</p>
<p><strong>Batch-learning</strong>：批学习</p>
<p>SVM定义了完整的损失函数，通过优化损失函数来得到模型参数</p>
<p><strong>Online-learning</strong> : 在线学习</p>
<p>感知机直接给出训练算法，通过数据集中每一个样本的迭代得到参数。</p>
<h2 id="3-2-Multi-class-Classification"><a href="#3-2-Multi-class-Classification" class="headerlink" title="3.2 Multi-class Classification"></a>3.2 Multi-class Classification</h2><h3 id="3-2-1-Defining-output-based-features"><a href="#3-2-1-Defining-output-based-features" class="headerlink" title="3.2.1 Defining output-based features"></a>3.2.1 Defining output-based features</h3><p>基于输出的向量表示</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221002193639504-1666628527050-24.png" alt="image-20221002193639504"></p>
<p>将输出也作为特征加入到向量中</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221002193851332-1666628527050-25.png" alt="image-20221002193851332"></p>
<h3 id="3-2-2-Multi-class-SVM"><a href="#3-2-2-Multi-class-SVM" class="headerlink" title="3.2.2 Multi-class SVM"></a>3.2.2 Multi-class SVM</h3><p><img src="/2022/10/04/NLP-Westlake/image-20221002194255807-1666628527050-26.png" alt="image-20221002194255807"></p>
<p><strong>基于打分的多分类感知机</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221002194828864-1666628527050-27.png" alt="image-20221002194828864"></p>
<p>正样本和负样本的差值 &gt;&#x3D; 1</p>
<h3 id="3-3-3-Multi-class-perceptron"><a href="#3-3-3-Multi-class-perceptron" class="headerlink" title="3.3.3 Multi-class perceptron"></a>3.3.3 Multi-class perceptron</h3><p><img src="/2022/10/04/NLP-Westlake/image-20221002195201349-1666628527050-28.png" alt="image-20221002195201349"></p>
<p>加上正确的特征向量 v(xi,ci) ，减去错误的特征向量 v(xi,zi)</p>
<p><strong>证明过程:</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221002195444476-1666628527050-29.png" alt="image-20221002195444476"></p>
<h2 id="3-3-Discriminative-Models-and-Features"><a href="#3-3-Discriminative-Models-and-Features" class="headerlink" title="3.3 Discriminative Models and Features"></a>3.3 Discriminative Models and Features</h2><p><strong>判别式模型</strong></p>
<ul>
<li><p>SVM 和 感知机 是判别式模型</p>
<p>让正例得到的分数比负例的分数高，直接对正例和负例进行区分。</p>
</li>
<li><p>朴素贝叶斯模型是生成模型，计算输入与输出的共同概率</p>
</li>
<li><p>三者都是线性模型</p>
</li>
</ul>
<p><img src="/2022/10/04/NLP-Westlake/image-20221003123042744-1666628527050-30.png" alt="image-20221003123042744"></p>
<p><strong>判别式模型优点</strong> :  可以加入重叠特征</p>
<h4 id="Feature-templates-and-instances"><a href="#Feature-templates-and-instances" class="headerlink" title="Feature templates and instances"></a><strong>Feature templates and instances</strong></h4><p><img src="/2022/10/04/NLP-Westlake/image-20221003132424988-1666628527050-32.png" alt="image-20221003132424988"></p>
<h3 id="3-3-2-Dot-product-Form-of-Linear-Models"><a href="#3-3-2-Dot-product-Form-of-Linear-Models" class="headerlink" title="3.3.2 Dot-product Form of Linear Models"></a>3.3.2 Dot-product Form of Linear Models</h3><p><strong>线性模型的统一表达模式</strong></p>
<ul>
<li>Given an input x，its score</li>
</ul>
<p>  <img src="/2022/10/04/NLP-Westlake/image-20221003132808497-1666628527050-31.png" alt="image-20221003132808497"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221003132833897-1666628527050-34.png" alt="image-20221003132833897"></p>
<h3 id="3-3-3-Separability-and-Generalizability"><a href="#3-3-3-Separability-and-Generalizability" class="headerlink" title="3.3.3 Separability and Generalizability"></a>3.3.3 Separability and Generalizability</h3><h5 id="可分性和泛化性"><a href="#可分性和泛化性" class="headerlink" title="可分性和泛化性"></a><strong>可分性和泛化性</strong></h5><ul>
<li><strong>特征工程</strong> : 定义一个有用的特征集合<ul>
<li>特征越多 信息越多</li>
<li>better designed 特征工程有更好的可分性</li>
</ul>
</li>
<li><strong>Separability 可分性</strong><ul>
<li>线性可分</li>
</ul>
</li>
<li><strong>Generalization 泛化性</strong><ul>
<li>overfitting</li>
<li>underfittting</li>
</ul>
</li>
</ul>
<h5 id="线性不可分问题"><a href="#线性不可分问题" class="headerlink" title="线性不可分问题"></a>线性不可分问题</h5><p>设置一个松弛变量</p>
<p>最小化松弛变量</p>
<h5 id="二分类线性不可分SVM问题"><a href="#二分类线性不可分SVM问题" class="headerlink" title="二分类线性不可分SVM问题"></a>二分类线性不可分SVM问题</h5><p><img src="/2022/10/04/NLP-Westlake/image-20221003135909337-1666628527050-33.png" alt="image-20221003135909337"></p>
<p>C 为超参</p>
<h5 id="多分类线性不可分SVM问题"><a href="#多分类线性不可分SVM问题" class="headerlink" title="多分类线性不可分SVM问题"></a>多分类线性不可分SVM问题</h5><p><img src="/2022/10/04/NLP-Westlake/image-20221003140320704-1666628527050-35.png" alt="image-20221003140320704"></p>
<h5 id="感知机线性不可分"><a href="#感知机线性不可分" class="headerlink" title="感知机线性不可分"></a>感知机线性不可分</h5><p>仍然可用</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221003140555278-1666628527050-36.png" alt="image-20221003140555278"></p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p><img src="/2022/10/04/NLP-Westlake/image-20221003140745816-1666628527050-37.png" alt="image-20221003140745816"></p>
<h1 id="chapter-4"><a href="#chapter-4" class="headerlink" title="chapter 4"></a>chapter 4</h1><h2 id="4-1-Log-Linear-Models"><a href="#4-1-Log-Linear-Models" class="headerlink" title="4.1 Log-Linear Models"></a>4.1 Log-Linear Models</h2><p>对数线性模型</p>
<h3 id="Review-linear-models"><a href="#Review-linear-models" class="headerlink" title="Review linear models"></a>Review linear models</h3><ul>
<li>Input x &#x3D; w<del>1</del>,w<del>2</del>,…,w<del>n</del>,output class c</li>
</ul>
<p><img src="/2022/10/04/NLP-Westlake/image-20221004163416870-1666628527050-38.png" alt="image-20221004163416870"></p>
<p><strong>判别式的线性模型</strong></p>
<ul>
<li><p>the general form：</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221004165706198-1666628527050-39.png" alt="image-20221004165706198"></p>
</li>
<li><p>advantage over generative models</p>
<ul>
<li>特征定义更加灵活</li>
<li>最小预测误差</li>
</ul>
</li>
<li><p>disadvantage</p>
<ul>
<li>结果不是概率值</li>
</ul>
</li>
</ul>
<h3 id="4-1-1-Training-binary-Log-linear-Models"><a href="#4-1-1-Training-binary-Log-linear-Models" class="headerlink" title="4.1.1 Training binary Log-linear Models"></a>4.1.1 Training binary Log-linear Models</h3><p><img src="/2022/10/04/NLP-Westlake/image-20221004170308712-1666628527050-40.png" alt="image-20221004170308712"></p>
<p>目标 : 将原来负无穷到正无穷的输出映射到0到1的概率值</p>
<h5 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h5><p><img src="/2022/10/04/NLP-Westlake/image-20221004184313222-1666628527051-47.png" alt="image-20221004184313222"></p>
<h5 id="Sigmoid-function"><a href="#Sigmoid-function" class="headerlink" title="Sigmoid function"></a>Sigmoid function</h5><p><img src="/2022/10/04/NLP-Westlake/image-20221004184710100-1666628527050-41.png" alt="image-20221004184710100"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221004202938705-1666628527050-42.png" alt="image-20221004202938705"></p>
<h5 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h5><p><img src="/2022/10/04/NLP-Westlake/image-20221004204134439-1666628527050-43.png" alt="image-20221004204134439"></p>
<h5 id="随机梯度下降法SGD"><a href="#随机梯度下降法SGD" class="headerlink" title="随机梯度下降法SGD"></a>随机梯度下降法SGD</h5><p><img src="/2022/10/04/NLP-Westlake/image-20221004204540158-1666628527050-44.png" alt="image-20221004204540158"></p>
<p>以一个训练样本为单位计算梯度，不用所有训练集来每次更新参数值</p>
<p><strong>对数线性二分类随机梯度法</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/../../../Desktop/file/ywj/Aispeech-ywj/%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/%E7%AC%94%E8%AE%B0/NLP-Westlake/image-20221004205102711.png" alt="image-20221004205102711"></p>
<h3 id="4-1-2-Training-multi-class-log-linear-models"><a href="#4-1-2-Training-multi-class-log-linear-models" class="headerlink" title="4.1.2 Training multi-class log-linear models"></a>4.1.2 Training multi-class log-linear models</h3><p><img src="/2022/10/04/NLP-Westlake/image-20221004213356396-1666628527051-45.png" alt="image-20221004213356396"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221004213826312-1666628527051-49.png" alt="image-20221004213826312"></p>
<h5 id="Mini-batch-SGD"><a href="#Mini-batch-SGD" class="headerlink" title="Mini-batch SGD"></a>Mini-batch SGD</h5><p>将所有训练数据切成m个batch</p>
<p><img src="/2022/10/04/NLP-Westlake/../../../Desktop/file/ywj/Aispeech-ywj/%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/%E7%AC%94%E8%AE%B0/NLP-Westlake/image-20221004214007325.png" alt="image-20221004214007325"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221004214227743-1666628527051-46.png" alt="image-20221004214227743"></p>
<h3 id="4-1-3-Using-log-linear-models-for-classification"><a href="#4-1-3-Using-log-linear-models-for-classification" class="headerlink" title="4.1.3 Using log-linear models for classification"></a>4.1.3 Using log-linear models for classification</h3><p><img src="/2022/10/04/NLP-Westlake/image-20221004220922092-1666628527051-48.png" alt="image-20221004220922092"></p>
<h5 id="Comparing-with-二分类感知机"><a href="#Comparing-with-二分类感知机" class="headerlink" title="Comparing with 二分类感知机"></a>Comparing with 二分类感知机</h5><p><img src="/2022/10/04/NLP-Westlake/image-20221004221711899.png" alt="image-20221004221711899"></p>
<p><strong>不同 :</strong> </p>
<p>感知机只对错误的样本对参数进行更新，对数线性对所有样本进行参数更新。</p>
<p>感知机的参数更新比较简单，对数线性模型的参数更新更加细粒度，导致最后产生一个概率。</p>
<h2 id="4-2-SGD-training-of-SVMs"><a href="#4-2-SGD-training-of-SVMs" class="headerlink" title="4.2 SGD training of SVMs"></a>4.2 SGD training of SVMs</h2><h3 id="4-2-1-Binary-classification"><a href="#4-2-1-Binary-classification" class="headerlink" title="4.2.1 Binary classification"></a>4.2.1 Binary classification</h3><p><img src="/2022/10/04/NLP-Westlake/image-20221004225041037-1666628527051-50.png" alt="image-20221004225041037"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221004225800479-1666628527051-51.png" alt="image-20221004225800479"></p>
<p><strong>Multi-class classification SVM</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221004230228385-1666628527051-52.png" alt="image-20221004230228385"></p>
<h3 id="Perceptron-vs-SVM-with-SGD"><a href="#Perceptron-vs-SVM-with-SGD" class="headerlink" title="Perceptron vs SVM-with-SGD"></a>Perceptron vs SVM-with-SGD</h3><p><strong>Binary</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221004231033840-1666628527051-53.png" alt="image-20221004231033840"></p>
<p>不同 : </p>
<p>​	更新的判断不同 </p>
<p>​	svm始终减去一项θ</p>
<p>​     svm有步长α</p>
<p><strong>Multi-class</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221004231420718.png" alt="image-20221004231420718"></p>
<h2 id="4-3-A-Generalizaed-Linear-Model-for-classification"><a href="#4-3-A-Generalizaed-Linear-Model-for-classification" class="headerlink" title="4.3 A Generalizaed Linear Model for classification"></a>4.3 A Generalizaed Linear Model for classification</h2><ul>
<li>Discriminative models<ul>
<li>感知机</li>
<li>SVM</li>
<li>对数线性模型</li>
</ul>
</li>
<li>Model form identical 模型形式一致</li>
</ul>
<p><img src="/2022/10/04/NLP-Westlake/image-20221006150341327-1666628527051-54.png" alt="image-20221006150341327"></p>
<ul>
<li>判别方式一致性</li>
</ul>
<p><img src="/2022/10/04/NLP-Westlake/image-20221006150404262-1666628527051-55.png" alt="image-20221006150404262"></p>
<p>​	输出得分最高的一类</p>
<h3 id="4-3-1-Unified-Online-Training"><a href="#4-3-1-Unified-Online-Training" class="headerlink" title="4.3.1 Unified Online Training"></a>4.3.1 Unified Online Training</h3><p><strong>在SGD优化的背景下，三者的训练过程大致相似</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221006150814514-1666628527051-56.png" alt="image-20221006150814514"></p>
<p><strong>参数的更新方式不同</strong></p>
<p><strong>二分类</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221006150940811-1666628527051-57.png" alt="image-20221006150940811"></p>
<p><strong>多分类</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221006150953698-1666628527051-58.png" alt="image-20221006150953698"></p>
<h3 id="4-3-2-Loss-function"><a href="#4-3-2-Loss-function" class="headerlink" title="4.3.2 Loss function"></a>4.3.2 Loss function</h3><p><strong>导致上述的参数更新方式不同的原因是三个模型的优化目标函数不同</strong></p>
<p><strong>优化目标函数 ~~ 损失函数</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221006151320353-1666628527051-59.png" alt="image-20221006151320353"></p>
<p><strong>对于感知机</strong></p>
<p>​		最小化错误的输出的分数和正确的分数之间的分差</p>
<p><strong>SVM</strong></p>
<p>​		在感知机的基础上多出一项最小化 λθ</p>
<p><strong>对数线性模型</strong></p>
<p>​		最小化 负的数据的似然</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221006151846609-1666628527051-60.png" alt="image-20221006151846609"></p>
<h3 id="4-4-1-Comparing-model-performances"><a href="#4-4-1-Comparing-model-performances" class="headerlink" title="4.4.1 Comparing model performances"></a>4.4.1 Comparing model performances</h3><p><strong>significance test</strong></p>
<p>假设两个模型的性能没有差别 零假设 null hypothesis</p>
<p>p&lt;0.05  假设不成立</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221006163655149.png" alt="image-20221006163655149"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221006163931533-1666628527051-61.png" alt="image-20221006163931533"></p>
<h2 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h2><p><img src="/2022/10/04/NLP-Westlake/image-20221006164225289-1666628527051-62.png" alt="image-20221006164225289"></p>
<h1 id="chapter-5"><a href="#chapter-5" class="headerlink" title="chapter 5"></a>chapter 5</h1><h3 id="5-1-1-Information-and-entropy"><a href="#5-1-1-Information-and-entropy" class="headerlink" title="5.1.1 Information and entropy"></a>5.1.1 Information and entropy</h3><p>一个随机事件有n个等同的可能</p>
<p>需要 log<del>2</del>n 位来表示</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221007230900867-1666628527051-63.png" alt="image-20221007230900867"></p>
<h5 id="Entropy-熵"><a href="#Entropy-熵" class="headerlink" title="Entropy 熵"></a>Entropy 熵</h5><p>熵是随机变量所有不同的取值所含有的信息量的加权平均</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221007231053730-1666628527051-64.png" alt="image-20221007231053730"></p>
<h3 id="5-1-2-最大熵模型"><a href="#5-1-2-最大熵模型" class="headerlink" title="5.1.2 最大熵模型"></a>5.1.2 最大熵模型</h3><p><strong>奥卡姆剃刀</strong></p>
<p><strong>最大熵模型</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221007232603784-1666628527051-65.png" alt="image-20221007232603784"></p>
<p><strong>拉格朗日求解</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221007232632864-1666628527051-66.png" alt="image-20221007232632864"></p>
<h3 id="5-1-3-使用最大熵训练模型"><a href="#5-1-3-使用最大熵训练模型" class="headerlink" title="5.1.3 使用最大熵训练模型"></a>5.1.3 使用最大熵训练模型</h3><h5 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h5><p><img src="/2022/10/04/NLP-Westlake/image-20221007233139371-1666628527051-67.png" alt="image-20221007233139371"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221015212207641-1666628527051-68.png" alt="image-20221015212207641"></p>
<h3 id="5-2-1-KL-divergence-KL散度"><a href="#5-2-1-KL-divergence-KL散度" class="headerlink" title="5.2.1 KL-divergence KL散度"></a>5.2.1 KL-divergence KL散度</h3><p> <img src="/2022/10/04/NLP-Westlake/image-20221017121821430-1666628527051-69.png" alt="image-20221017121821430"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221017130050466-1666628527051-70.png" alt="image-20221017130050466"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221017130546965-1666628527051-71.png" alt="image-20221017130546965"></p>
<h3 id="5-2-2-交叉熵"><a href="#5-2-2-交叉熵" class="headerlink" title="5.2.2 交叉熵"></a>5.2.2 交叉熵</h3><p><img src="/2022/10/04/NLP-Westlake/image-20221017130727310-1666628527051-72.png" alt="image-20221017130727310"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221017131312477-1666628527051-73.png" alt="image-20221017131312477"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221017131433443-1666628527051-74.png" alt="image-20221017131433443"></p>
<h3 id="5-2-3-Model-perplexity-困惑度"><a href="#5-2-3-Model-perplexity-困惑度" class="headerlink" title="5.2.3 Model perplexity 困惑度"></a>5.2.3 Model perplexity 困惑度</h3><p><img src="/2022/10/04/NLP-Westlake/image-20221017131617762-1666628527051-75.png" alt="image-20221017131617762"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221017131744720-1666628527051-76.png" alt="image-20221017131744720"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221017131847932-1666628527051-77.png" alt="image-20221017131847932"></p>
<h3 id="5-3-Mutual-information-互信息"><a href="#5-3-Mutual-information-互信息" class="headerlink" title="5.3 Mutual information 互信息"></a>5.3 Mutual information 互信息</h3><p>互信息研究了两个随机变量之间的联系</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221017150504790-1666628527051-78.png" alt="image-20221017150504790"></p>
<p>熵 : </p>
<p>​	衡量编码一个随机变量所需的比特数，</p>
<p>交叉熵:</p>
<p>​	衡量两个不同分布之间的相互关联</p>
<p>​    熵是用最优的编码方式，也就是最少的比特数来编码一个随机事件，交叉熵是用另一种分布方式编码。</p>
<p>​	交叉熵 &gt; 熵</p>
<p> <strong>互信息</strong> : 已知一个随机变量后，编码另一个随机变量可以节省的比特数</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221017153622830-1666628527051-79.png" alt="image-20221017153622830"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221017153822650-1666628527051-80.png" alt="image-20221017153822650"></p>
<h3 id="5-3-1-Pointwise-mutual-information-点互信息"><a href="#5-3-1-Pointwise-mutual-information-点互信息" class="headerlink" title="5.3.1 Pointwise mutual information 点互信息"></a>5.3.1 Pointwise mutual information 点互信息</h3><p><img src="/2022/10/04/NLP-Westlake/image-20221017154444202-1666628527051-81.png" alt="image-20221017154444202"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221017164713863-1666628527051-82.png" alt="image-20221017164713863"></p>
<p>互信息衡量了两个随机变量之间的关联，而点互信息（PMI）关注两个随机变量之间具体取值的关联。 </p>
<p>点互信息关注于随机变量的具体取值，互信息关注于随机变量的平均概念。 </p>
<p>PMI 越大，两个随机变量取值之间的关联就越大</p>
<h3 id="5-3-2-Using-PMI"><a href="#5-3-2-Using-PMI" class="headerlink" title="5.3.2 Using PMI"></a>5.3.2 Using PMI</h3><p>​	情感词典、习惯用语的抽取和特征抽取。</p>
<ul>
<li><h6 id="Sentiment-lexicons"><a href="#Sentiment-lexicons" class="headerlink" title="Sentiment lexicons"></a>Sentiment lexicons</h6></li>
</ul>
<p><img src="/2022/10/04/NLP-Westlake/image-20221017170306833-1666628527051-83.png" alt="image-20221017170306833"></p>
<ul>
<li><h6 id="Collocatopn-extraction"><a href="#Collocatopn-extraction" class="headerlink" title="Collocatopn extraction"></a>Collocatopn extraction</h6></li>
</ul>
<p>​	习惯用语</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221017170713916-1666628527051-84.png" alt="image-20221017170713916"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221017170735047-1666628527051-85.png" alt="image-20221017170735047"></p>
<ul>
<li><h6 id="Feature-Selection"><a href="#Feature-Selection" class="headerlink" title="Feature Selection"></a>Feature Selection</h6></li>
</ul>
<p><img src="/2022/10/04/NLP-Westlake/image-20221017171048743-1666628527051-86.png" alt="image-20221017171048743"></p>
<h3 id="词的向量表示"><a href="#词的向量表示" class="headerlink" title="词的向量表示"></a>词的向量表示</h3><p>为什么要用向量表示词？</p>
<ul>
<li>useful for measuring semantic correlations 语义关联</li>
</ul>
<p>one-hot 编码</p>
<p>context  基于上下文编码</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221018201827997-1666628527051-87.png" alt="image-20221018201827997"></p>
<p>用 PMI 代替上下文</p>
<p>类似于 TF-idf</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221018202110954-1666628527051-88.png" alt="image-20221018202110954"></p>
<p>PPMI 将负的PMI置零，去除互斥的词</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221018202454631-1666628527051-89.png" alt="image-20221018202454631"></p>
<h3 id="Summary-2"><a href="#Summary-2" class="headerlink" title="Summary"></a>Summary</h3><p><img src="/2022/10/04/NLP-Westlake/image-20221018202609727-1666628527051-90.png" alt="image-20221018202609727"></p>
<h1 id="chapter-6"><a href="#chapter-6" class="headerlink" title="chapter 6"></a>chapter 6</h1><h3 id="6-1-Expectation-Maximization"><a href="#6-1-Expectation-Maximization" class="headerlink" title="6.1  Expectation Maximization"></a>6.1  Expectation Maximization</h3><h4 id="6-1-1-Introduction"><a href="#6-1-1-Introduction" class="headerlink" title="6.1.1 Introduction"></a>6.1.1 Introduction</h4><h5 id="Hidden-Variables"><a href="#Hidden-Variables" class="headerlink" title="Hidden Variables"></a>Hidden Variables</h5><p>在训练数据中看不到的数据</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221018202957428-1666628527051-95.png" alt="image-20221018202957428"></p>
<p>缺少标签信息(隐变量，如上图中中英文的对齐关系为隐变量)MLE无法使用</p>
<h5 id="Dealing-with-Hidden-Variables"><a href="#Dealing-with-Hidden-Variables" class="headerlink" title="Dealing with Hidden Variables"></a>Dealing with Hidden Variables</h5><p><img src="/2022/10/04/NLP-Westlake/image-20221018210517330-1666628527051-91.png" alt="image-20221018210517330"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221018211534302-1666628527051-92.png" alt="image-20221018211534302"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221018211624575-1666628527051-93.png" alt="image-20221018211624575"></p>
<p>THREE WAYS to estimate hidden variable counts</p>
<ul>
<li>Hard max</li>
<li>soft probabilities<ul>
<li>把一个文档按概率分成0.6的a，0.4的b。。等等</li>
</ul>
</li>
<li>sampling<ul>
<li>按概率选择</li>
</ul>
</li>
</ul>
<p><img src="/2022/10/04/NLP-Westlake/image-20221018221847991-1666628527051-94.png" alt="image-20221018221847991"></p>
<p>以隐变量的角度看待k-means</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221018222326525-1666628527052-96.png" alt="image-20221018222326525"></p>
<h5 id="hard-EM-算法"><a href="#hard-EM-算法" class="headerlink" title="hard-EM 算法"></a>hard-EM 算法</h5><p><img src="/2022/10/04/NLP-Westlake/image-20221018222859775-1666628527052-97.png" alt="image-20221018222859775"></p>
<p><strong>Two steps for EM</strong></p>
<ul>
<li>Expectation (E-step)<ul>
<li>根据模型参数计算隐变量的概率<ul>
<li>hard max</li>
<li>soft probabilities</li>
<li>sampling</li>
</ul>
</li>
</ul>
</li>
<li>Maximization (M-step)<ul>
<li>根据隐变量估计参数</li>
</ul>
</li>
</ul>
<p><strong>K-means是一种 hard EM 算法</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221018223244443-1666628527052-98.png" alt="image-20221018223244443"></p>
<p><strong>但hard-EM是最大概率化的过程</strong></p>
<p><strong>因此需要把k-means中的距离进行概率化操作</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221018223417466-1666628527052-99.png" alt="image-20221018223417466"></p>
<p><strong>最优化向量距离转化为最大化概率值</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221018223453208-1666628527052-100.png" alt="image-20221018223453208"></p>
<p><strong>K-means 与 hard-EM的联系</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221018224008883-1666628527052-101.png" alt="image-20221018224008883"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221018224113586-1666628527052-102.png" alt="image-20221018224113586"></p>
<h4 id="6-1-2-EM"><a href="#6-1-2-EM" class="headerlink" title="6.1.2 EM"></a>6.1.2 EM</h4><p>使用 soft-probabilities 计算隐函数</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221019003733876-1666628527052-103.png" alt="image-20221019003733876"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221019014134737-1666628527052-104.png" alt="image-20221019014134737"></p>
<h4 id="6-1-3-MLE-AND-EM"><a href="#6-1-3-MLE-AND-EM" class="headerlink" title="6.1.3 MLE AND EM"></a>6.1.3 MLE AND EM</h4><p><img src="/2022/10/04/NLP-Westlake/image-20221019014845714-1666628527052-105.png" alt="image-20221019014845714"></p>
<p>EM 是 MLE的一般形式</p>
<h3 id="6-2-Using-EM"><a href="#6-2-Using-EM" class="headerlink" title="6.2 Using EM"></a>6.2 Using EM</h3><p><img src="/2022/10/04/NLP-Westlake/image-20221021000622639-1666628527052-106.png" alt="image-20221021000622639"></p>
<h4 id="6-2-1-unsupervised-naive-bayes-model"><a href="#6-2-1-unsupervised-naive-bayes-model" class="headerlink" title="6.2.1 unsupervised naive bayes model"></a>6.2.1 unsupervised naive bayes model</h4><p><img src="/2022/10/04/NLP-Westlake/image-20221021001014101-1666628527052-107.png" alt="image-20221021001014101"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221021001122812-1666628527052-108.png" alt="image-20221021001122812"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221021001813414-1666628527052-109.png" alt="image-20221021001813414"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221021001958934-1666628527052-110.png" alt="image-20221021001958934"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221021002007628-1666628527052-111.png" alt="image-20221021002007628"></p>
<h4 id="6-2-2-IBM-Model1"><a href="#6-2-2-IBM-Model1" class="headerlink" title="6.2.2 IBM Model1"></a>6.2.2 IBM Model1</h4><p><img src="/2022/10/04/NLP-Westlake/image-20221021003435297-1666628527052-112.png" alt="image-20221021003435297"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221021004131041-1666628527052-113.png" alt="image-20221021004131041"></p>
<p> 词对齐是隐变量</p>
<p> <img src="/2022/10/04/NLP-Westlake/image-20221021005805885-1666628527052-114.png" alt="image-20221021005805885"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221021010419619-1666628527052-115.png" alt="image-20221021010419619"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221021010433326-1666628527052-116.png" alt="image-20221021010433326"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221021010618556-1666628527052-117.png" alt="image-20221021010618556"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221021010745999-1666628527052-118.png" alt="image-20221021010745999"></p>
<h4 id="6-2-3-Probabilistic-latent-semantic-anlysis"><a href="#6-2-3-Probabilistic-latent-semantic-anlysis" class="headerlink" title="6.2.3 Probabilistic latent semantic anlysis"></a>6.2.3 Probabilistic latent semantic anlysis</h4><p>概率潜在语义分析  PLSA</p>
<p>概率潜在语义分析模型是一种对文档的潜在语义进行分析和表示的模型，它可以将文档转换成向量。</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221021231732220-1666628527052-119.png" alt="image-20221021231732220"></p>
<p>用 tf-idf 表示一个文章过于稀疏 </p>
<p><strong>PLSA</strong> <strong>用主题代替词</strong> </p>
<p>主题作为隐变量</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221021231925535-1666628527052-120.png" alt="image-20221021231925535"></p>
<p>训练集的生成过程</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221021232415167-1666628527052-121.png" alt="image-20221021232415167"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221021232627619-1666628527052-122.png" alt="image-20221021232627619"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221021233223004-1666628527052-123.png" alt="image-20221021233223004"></p>
<h4 id="EM算法的推导"><a href="#EM算法的推导" class="headerlink" title="EM算法的推导"></a>EM算法的推导</h4><p><img src="/2022/10/04/NLP-Westlake/image-20221022000341229-1666628527052-124.png" alt="image-20221022000341229"></p>
<p><strong>简森不等式</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221022000545562-1666628527052-125.png" alt="image-20221022000545562"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221022000740607-1666628527052-126.png" alt="image-20221022000740607"></p>
<p> 由 KL 散度推出 EM</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221022001304474-1666628527052-127.png" alt="image-20221022001304474"></p>
<h3 id="Summary-3"><a href="#Summary-3" class="headerlink" title="Summary"></a>Summary</h3><p><img src="/2022/10/04/NLP-Westlake/image-20221022001827461-1666628527052-128.png" alt="image-20221022001827461"></p>
<h1 id="chapter-7"><a href="#chapter-7" class="headerlink" title="chapter 7"></a>chapter 7</h1><h3 id="7-1-Sequence-Labelling"><a href="#7-1-Sequence-Labelling" class="headerlink" title="7.1 Sequence Labelling"></a>7.1 Sequence Labelling</h3><p><strong>序列标注</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221022005248436-1666628527052-129.png" alt="image-20221022005248436"></p>
<p><strong>词性标注</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221022005554840-1666628527052-130.png" alt="image-20221022005554840"></p>
<p>​    <strong>把词性标注问题看作单独的分类问题</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221022005838839-1666628527052-131.png" alt="image-20221022005838839"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221022010344493-1666628527052-132.png" alt="image-20221022010344493"></p>
<h3 id="7-2-HMM-隐马尔可夫模型"><a href="#7-2-HMM-隐马尔可夫模型" class="headerlink" title="7.2 HMM 隐马尔可夫模型"></a>7.2 HMM 隐马尔可夫模型</h3><p>生成式模型</p>
<p>给定句子W，生成词序列T的概率</p>
<p>隐马尔可夫模型（Hidden Markov Model，HMM）是一种生成模型。 对于给定输入，HMM对输出的条件概率进行建模，可以用于序列标注。</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221022010640690-1666628527052-133.png" alt="image-20221022010640690"></p>
<p><strong>构造模型</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221022010849015-1666628527052-134.png" alt="image-20221022010849015"></p>
<p><strong>运用概率的链式法则</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221022010923208-1666628527052-135.png" alt="image-20221022010923208"></p>
<p><strong>运用马尔可夫假设</strong> 只和前面n个词有关</p>
<p><strong>对另一项使用同样方法</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221022011308997-1666628527052-136.png" alt="image-20221022011308997"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221022011343074-1666628527052-137.png" alt="image-20221022011343074"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221022011500750-1666628527052-138.png" alt="image-20221022011500750"></p>
<p><strong>Summary</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221022011627093-1666628527052-139.png" alt="image-20221022011627093"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221022011716811-1666628527052-140.png" alt="image-20221022011716811"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221022011737724-1666628527052-141.png" alt="image-20221022011737724"></p>
<p>两个参数 : 发射概率，转移概率 </p>
<h4 id="7-2-1-Training"><a href="#7-2-1-Training" class="headerlink" title="7.2.1 Training"></a>7.2.1 Training</h4><p><img src="/2022/10/04/NLP-Westlake/image-20221022012145804-1666628527052-142.png" alt="image-20221022012145804"></p>
<h4 id="7-2-2-Decoding"><a href="#7-2-2-Decoding" class="headerlink" title="7.2.2 Decoding"></a>7.2.2 Decoding</h4><p>解码 : 给定模型和输入，找到最有可能的输出是什么</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221022225509854-1666628527052-143.png" alt="image-20221022225509854"></p>
<p><strong>动态规划</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221022225814160-1666628527052-144.png" alt="image-20221022225814160"></p>
<p>得到最高得分的 T^^^ <del>i-1</del> </p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221022230503931-1666628527052-145.png" alt="image-20221022230503931"></p>
<p>固定 t<del>i-1</del> 找到最大的 P（t | t<del>i-1</del>）</p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221022230607295-1666628527052-146.png" alt="image-20221022230607295"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221023004746272-1666628527052-147.png" alt="image-20221023004746272"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221023005231211-1666628527052-148.png" alt="image-20221023005231211"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221023005345148-1666628527052-149.png" alt="image-20221023005345148"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221023005455727-1666628527052-150.png" alt="image-20221023005455727"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221023005518712-1666628527052-151.png" alt="image-20221023005518712"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221023005605196-1666628527052-152.png" alt="image-20221023005605196"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221023005614961-1666628527052-153.png" alt="image-20221023005614961"></p>
<h3 id="7-3-计算边缘概率"><a href="#7-3-计算边缘概率" class="headerlink" title="7.3 计算边缘概率"></a>7.3 计算边缘概率</h3><p>回顾    <strong>打分    训练     解码</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221023014328397-1666628527053-154.png" alt="image-20221023014328397"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221023014705862-1666628527053-155.png" alt="image-20221023014705862"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221023015016969-1666628527053-156.png" alt="image-20221023015016969"></p>
<p>可以分解成两项</p>
<p><strong>前项</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221023015225776-1666628527053-157.png" alt="image-20221023015225776"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221023015605985-1666628527053-158.png" alt="image-20221023015605985"></p>
<p><strong>后项</strong></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221023015652160-1666628527053-159.png" alt="image-20221023015652160"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221023015932606-1666628527053-160.png" alt="image-20221023015932606"></p>
<h3 id="7-4-无监督训练HMM"><a href="#7-4-无监督训练HMM" class="headerlink" title="7.4 无监督训练HMM"></a>7.4 无监督训练HMM</h3><p><img src="/2022/10/04/NLP-Westlake/image-20221025001200021-1666628527053-161.png" alt="image-20221025001200021"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221025001359083-1666628527053-162.png" alt="image-20221025001359083"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221025001603936-1666628527053-163.png" alt="image-20221025001603936"></p>
<p><img src="/2022/10/04/NLP-Westlake/image-20221025001911763-1666628527053-164.png" alt="image-20221025001911763"></p>
<p>没懂</p>
<h3 id="性能指标"><a href="#性能指标" class="headerlink" title="性能指标"></a>性能指标</h3><p><strong>Precision</strong> <strong>查准率</strong><br>$$<br>Precision &#x3D; \frac {True \  positive}{predicted \ positive } &#x3D; \frac {True \ positive} {True \ positive + False \ positive}<br>$$</p>
<p><strong>Recall 召回率</strong><br>$$<br>Precision &#x3D; \frac {True \  positive}{\ actual \ positive } &#x3D; \frac {True \ positive} {True \ positive + False \ negative}<br>$$</p>
<p><strong>F-score</strong><br>$$<br>F-{Score} &#x3D;  \frac{2 * P * R}{P + R}<br>$$<br><img src="/2022/10/04/NLP-Westlake/image-20221025003528211.png" alt="image-20221025003528211"></p>
<p>$$<br>\frac{d}{dW} ( \frac{1}{2}\frac{\lambda}{m} W^2) &#x3D; \frac{\lambda}{m} W<br>$$</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E7%AC%94%E8%AE%B0/" rel="tag"># 笔记</a>
              <a href="/tags/NLP/" rel="tag"># NLP</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/10/04/ML-%E5%90%B4%E6%81%A9%E8%BE%BE/" rel="prev" title="Machine Learning   --wuenda">
      <i class="fa fa-chevron-left"></i> Machine Learning   --wuenda
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/10/04/%E5%B7%A5%E4%BD%9C%E6%97%A5%E5%BF%97/" rel="next" title="工作日志">
      工作日志 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#chapter-1"><span class="nav-number">1.</span> <span class="nav-text">chapter 1</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-what-is-nlp"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 what is  nlp</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E8%87%AA%E5%8A%A8%E7%9A%84%E7%90%86%E8%A7%A3%E5%92%8C%E7%94%9F%E6%88%90%E4%BA%BA%E7%B1%BB%E8%AF%AD%E8%A8%80"><span class="nav-number">1.1.0.0.0.1.</span> <span class="nav-text">自动的理解和生成人类语言</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Main-approaches"><span class="nav-number">1.1.1.</span> <span class="nav-text">Main approaches</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Rule-based-symbolic-approach-%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">1.1.1.0.0.1.</span> <span class="nav-text">Rule-based(symbolic) approach  基于规则的方法</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-NLP-tasks"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 NLP  tasks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-1-Fundamental-Tasks"><span class="nav-number">1.2.1.</span> <span class="nav-text">1.2.1 Fundamental Tasks</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Syntactic-tasks-%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90%E4%BB%BB%E5%8A%A1"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">Syntactic tasks 句法分析任务</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Semantic-task-%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90%E4%BB%BB%E5%8A%A1"><span class="nav-number">1.2.1.2.</span> <span class="nav-text">Semantic task 语义分析任务</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Discourse-tasks-%E7%AF%87%E7%AB%A0%E5%88%86%E6%9E%90"><span class="nav-number">1.2.1.3.</span> <span class="nav-text">Discourse tasks 篇章分析</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-2-Information-extraction-%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96"><span class="nav-number">1.2.2.</span> <span class="nav-text">1.2.2 Information extraction 信息抽取</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Entities"><span class="nav-number">1.2.2.0.0.1.</span> <span class="nav-text">Entities</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-3-Text-Generation-Tasks"><span class="nav-number">1.2.3.</span> <span class="nav-text">1.2.3 Text Generation Tasks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-4-other-applications"><span class="nav-number">1.2.4.</span> <span class="nav-text">1.2.4 other applications</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-NLP-from-a-Machine-Learning-Perspective"><span class="nav-number">1.3.</span> <span class="nav-text">1.3 NLP from a Machine Learning Perspective</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Chapter-2"><span class="nav-number">2.</span> <span class="nav-text">Chapter 2</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-what-is-model"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 what is model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-1-MLE"><span class="nav-number">2.1.1.</span> <span class="nav-text">2.1.1 MLE</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-N-gram-Language-Models"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 N-gram Language Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-2-Bigram-Language-Models"><span class="nav-number">2.2.1.</span> <span class="nav-text">2.2.2 Bigram Language Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-3-Trigram-Language-Models-and-Beyond"><span class="nav-number">2.2.2.</span> <span class="nav-text">2.2.3 Trigram Language Models and Beyond</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Methods-to-address-sparsity"><span class="nav-number">2.2.3.</span> <span class="nav-text">Methods to address sparsity</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Log-probability-models"><span class="nav-number">2.2.4.</span> <span class="nav-text">Log-probability models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-4-Generative-Models"><span class="nav-number">2.2.5.</span> <span class="nav-text">2.2.4 Generative Models</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-Naive-Bayes-rule"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 Naive Bayes rule</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-2-Evaluating-a-Text-Classifier"><span class="nav-number">2.3.1.</span> <span class="nav-text">2.3.2 Evaluating a Text Classifier</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-3-Features"><span class="nav-number">2.3.2.</span> <span class="nav-text">2.3.3 Features</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Chapter-3"><span class="nav-number">3.</span> <span class="nav-text">Chapter 3</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-Representing-Documents-in-Vector-Spaces"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 Representing Documents in Vector Spaces</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Naive-Bayes-model"><span class="nav-number">3.1.0.1.</span> <span class="nav-text">Naive Bayes model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#stop-words"><span class="nav-number">3.1.0.2.</span> <span class="nav-text">stop words</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#TF-IDF-vector"><span class="nav-number">3.1.0.3.</span> <span class="nav-text">TF-IDF vector</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-1-Clustering"><span class="nav-number">3.1.1.</span> <span class="nav-text">3.1.1 Clustering</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%91%E9%87%8F%E4%B9%8B%E9%97%B4%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95"><span class="nav-number">3.1.1.1.</span> <span class="nav-text">向量之间距离计算方法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-2-K-means-clustering"><span class="nav-number">3.1.2.</span> <span class="nav-text">3.1.2 K-means clustering</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-3-Classification"><span class="nav-number">3.1.3.</span> <span class="nav-text">3.1.3 Classification</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-4-SVM"><span class="nav-number">3.1.4.</span> <span class="nav-text">3.1.4 SVM</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Support-vector"><span class="nav-number">3.1.4.0.1.</span> <span class="nav-text">Support vector</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SVM-Classifier"><span class="nav-number">3.1.4.1.</span> <span class="nav-text">SVM Classifier</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-5-Perceptron-%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="nav-number">3.1.5.</span> <span class="nav-text">3.1.5 Perceptron 感知机</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-Multi-class-Classification"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 Multi-class Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-1-Defining-output-based-features"><span class="nav-number">3.2.1.</span> <span class="nav-text">3.2.1 Defining output-based features</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-2-Multi-class-SVM"><span class="nav-number">3.2.2.</span> <span class="nav-text">3.2.2 Multi-class SVM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-3-Multi-class-perceptron"><span class="nav-number">3.2.3.</span> <span class="nav-text">3.3.3 Multi-class perceptron</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-Discriminative-Models-and-Features"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 Discriminative Models and Features</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Feature-templates-and-instances"><span class="nav-number">3.3.0.1.</span> <span class="nav-text">Feature templates and instances</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-2-Dot-product-Form-of-Linear-Models"><span class="nav-number">3.3.1.</span> <span class="nav-text">3.3.2 Dot-product Form of Linear Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-3-Separability-and-Generalizability"><span class="nav-number">3.3.2.</span> <span class="nav-text">3.3.3 Separability and Generalizability</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8F%AF%E5%88%86%E6%80%A7%E5%92%8C%E6%B3%9B%E5%8C%96%E6%80%A7"><span class="nav-number">3.3.2.0.1.</span> <span class="nav-text">可分性和泛化性</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E4%B8%8D%E5%8F%AF%E5%88%86%E9%97%AE%E9%A2%98"><span class="nav-number">3.3.2.0.2.</span> <span class="nav-text">线性不可分问题</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BA%8C%E5%88%86%E7%B1%BB%E7%BA%BF%E6%80%A7%E4%B8%8D%E5%8F%AF%E5%88%86SVM%E9%97%AE%E9%A2%98"><span class="nav-number">3.3.2.0.3.</span> <span class="nav-text">二分类线性不可分SVM问题</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%A4%9A%E5%88%86%E7%B1%BB%E7%BA%BF%E6%80%A7%E4%B8%8D%E5%8F%AF%E5%88%86SVM%E9%97%AE%E9%A2%98"><span class="nav-number">3.3.2.0.4.</span> <span class="nav-text">多分类线性不可分SVM问题</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%BA%BF%E6%80%A7%E4%B8%8D%E5%8F%AF%E5%88%86"><span class="nav-number">3.3.2.0.5.</span> <span class="nav-text">感知机线性不可分</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Summary"><span class="nav-number">3.4.</span> <span class="nav-text">Summary</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#chapter-4"><span class="nav-number">4.</span> <span class="nav-text">chapter 4</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-Log-Linear-Models"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 Log-Linear Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Review-linear-models"><span class="nav-number">4.1.1.</span> <span class="nav-text">Review linear models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-1-Training-binary-Log-linear-Models"><span class="nav-number">4.1.2.</span> <span class="nav-text">4.1.1 Training binary Log-linear Models</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Softmax"><span class="nav-number">4.1.2.0.1.</span> <span class="nav-text">Softmax</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Sigmoid-function"><span class="nav-number">4.1.2.0.2.</span> <span class="nav-text">Sigmoid function</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="nav-number">4.1.2.0.3.</span> <span class="nav-text">梯度下降法</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95SGD"><span class="nav-number">4.1.2.0.4.</span> <span class="nav-text">随机梯度下降法SGD</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-2-Training-multi-class-log-linear-models"><span class="nav-number">4.1.3.</span> <span class="nav-text">4.1.2 Training multi-class log-linear models</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Mini-batch-SGD"><span class="nav-number">4.1.3.0.1.</span> <span class="nav-text">Mini-batch SGD</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-3-Using-log-linear-models-for-classification"><span class="nav-number">4.1.4.</span> <span class="nav-text">4.1.3 Using log-linear models for classification</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Comparing-with-%E4%BA%8C%E5%88%86%E7%B1%BB%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="nav-number">4.1.4.0.1.</span> <span class="nav-text">Comparing with 二分类感知机</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-SGD-training-of-SVMs"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 SGD training of SVMs</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-1-Binary-classification"><span class="nav-number">4.2.1.</span> <span class="nav-text">4.2.1 Binary classification</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Perceptron-vs-SVM-with-SGD"><span class="nav-number">4.2.2.</span> <span class="nav-text">Perceptron vs SVM-with-SGD</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-3-A-Generalizaed-Linear-Model-for-classification"><span class="nav-number">4.3.</span> <span class="nav-text">4.3 A Generalizaed Linear Model for classification</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-1-Unified-Online-Training"><span class="nav-number">4.3.1.</span> <span class="nav-text">4.3.1 Unified Online Training</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-2-Loss-function"><span class="nav-number">4.3.2.</span> <span class="nav-text">4.3.2 Loss function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-1-Comparing-model-performances"><span class="nav-number">4.3.3.</span> <span class="nav-text">4.4.1 Comparing model performances</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Summary-1"><span class="nav-number">4.4.</span> <span class="nav-text">Summary</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#chapter-5"><span class="nav-number">5.</span> <span class="nav-text">chapter 5</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-1-Information-and-entropy"><span class="nav-number">5.0.1.</span> <span class="nav-text">5.1.1 Information and entropy</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Entropy-%E7%86%B5"><span class="nav-number">5.0.1.0.1.</span> <span class="nav-text">Entropy 熵</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-2-%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B"><span class="nav-number">5.0.2.</span> <span class="nav-text">5.1.2 最大熵模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-3-%E4%BD%BF%E7%94%A8%E6%9C%80%E5%A4%A7%E7%86%B5%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="nav-number">5.0.3.</span> <span class="nav-text">5.1.3 使用最大熵训练模型</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%9D%A1%E4%BB%B6%E7%86%B5"><span class="nav-number">5.0.3.0.1.</span> <span class="nav-text">条件熵</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-1-KL-divergence-KL%E6%95%A3%E5%BA%A6"><span class="nav-number">5.0.4.</span> <span class="nav-text">5.2.1 KL-divergence KL散度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-2-%E4%BA%A4%E5%8F%89%E7%86%B5"><span class="nav-number">5.0.5.</span> <span class="nav-text">5.2.2 交叉熵</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-3-Model-perplexity-%E5%9B%B0%E6%83%91%E5%BA%A6"><span class="nav-number">5.0.6.</span> <span class="nav-text">5.2.3 Model perplexity 困惑度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-Mutual-information-%E4%BA%92%E4%BF%A1%E6%81%AF"><span class="nav-number">5.0.7.</span> <span class="nav-text">5.3 Mutual information 互信息</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-1-Pointwise-mutual-information-%E7%82%B9%E4%BA%92%E4%BF%A1%E6%81%AF"><span class="nav-number">5.0.8.</span> <span class="nav-text">5.3.1 Pointwise mutual information 点互信息</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-2-Using-PMI"><span class="nav-number">5.0.9.</span> <span class="nav-text">5.3.2 Using PMI</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Sentiment-lexicons"><span class="nav-number">5.0.9.0.0.1.</span> <span class="nav-text">Sentiment lexicons</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Collocatopn-extraction"><span class="nav-number">5.0.9.0.0.2.</span> <span class="nav-text">Collocatopn extraction</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Feature-Selection"><span class="nav-number">5.0.9.0.0.3.</span> <span class="nav-text">Feature Selection</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%8D%E7%9A%84%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA"><span class="nav-number">5.0.10.</span> <span class="nav-text">词的向量表示</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Summary-2"><span class="nav-number">5.0.11.</span> <span class="nav-text">Summary</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#chapter-6"><span class="nav-number">6.</span> <span class="nav-text">chapter 6</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-Expectation-Maximization"><span class="nav-number">6.0.1.</span> <span class="nav-text">6.1  Expectation Maximization</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-1-Introduction"><span class="nav-number">6.0.1.1.</span> <span class="nav-text">6.1.1 Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Hidden-Variables"><span class="nav-number">6.0.1.1.1.</span> <span class="nav-text">Hidden Variables</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Dealing-with-Hidden-Variables"><span class="nav-number">6.0.1.1.2.</span> <span class="nav-text">Dealing with Hidden Variables</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#hard-EM-%E7%AE%97%E6%B3%95"><span class="nav-number">6.0.1.1.3.</span> <span class="nav-text">hard-EM 算法</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-2-EM"><span class="nav-number">6.0.1.2.</span> <span class="nav-text">6.1.2 EM</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-3-MLE-AND-EM"><span class="nav-number">6.0.1.3.</span> <span class="nav-text">6.1.3 MLE AND EM</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-Using-EM"><span class="nav-number">6.0.2.</span> <span class="nav-text">6.2 Using EM</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-1-unsupervised-naive-bayes-model"><span class="nav-number">6.0.2.1.</span> <span class="nav-text">6.2.1 unsupervised naive bayes model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-2-IBM-Model1"><span class="nav-number">6.0.2.2.</span> <span class="nav-text">6.2.2 IBM Model1</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-3-Probabilistic-latent-semantic-anlysis"><span class="nav-number">6.0.2.3.</span> <span class="nav-text">6.2.3 Probabilistic latent semantic anlysis</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#EM%E7%AE%97%E6%B3%95%E7%9A%84%E6%8E%A8%E5%AF%BC"><span class="nav-number">6.0.2.4.</span> <span class="nav-text">EM算法的推导</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Summary-3"><span class="nav-number">6.0.3.</span> <span class="nav-text">Summary</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#chapter-7"><span class="nav-number">7.</span> <span class="nav-text">chapter 7</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-Sequence-Labelling"><span class="nav-number">7.0.1.</span> <span class="nav-text">7.1 Sequence Labelling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-HMM-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B"><span class="nav-number">7.0.2.</span> <span class="nav-text">7.2 HMM 隐马尔可夫模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-2-1-Training"><span class="nav-number">7.0.2.1.</span> <span class="nav-text">7.2.1 Training</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-2-2-Decoding"><span class="nav-number">7.0.2.2.</span> <span class="nav-text">7.2.2 Decoding</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-%E8%AE%A1%E7%AE%97%E8%BE%B9%E7%BC%98%E6%A6%82%E7%8E%87"><span class="nav-number">7.0.3.</span> <span class="nav-text">7.3 计算边缘概率</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-4-%E6%97%A0%E7%9B%91%E7%9D%A3%E8%AE%AD%E7%BB%83HMM"><span class="nav-number">7.0.4.</span> <span class="nav-text">7.4 无监督训练HMM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87"><span class="nav-number">7.0.5.</span> <span class="nav-text">性能指标</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Moriarty</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">32</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://p7qnhh1nqx.feishu.cn/wiki/wikcnu4ROjCNXxoCk0jeOBmDwZc" title="https:&#x2F;&#x2F;p7qnhh1nqx.feishu.cn&#x2F;wiki&#x2F;wikcnu4ROjCNXxoCk0jeOBmDwZc" rel="noopener" target="_blank">工作日报</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://space.bilibili.com/14898606/favlist" title="https:&#x2F;&#x2F;space.bilibili.com&#x2F;14898606&#x2F;favlist" rel="noopener" target="_blank">学习资源</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Moriarty</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
